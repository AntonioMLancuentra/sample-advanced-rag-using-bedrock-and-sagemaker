{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fe8a71-6170-4caf-a3b9-e578c1c5f201",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context.\n",
    "\n",
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66330105-e1f4-46f3-9b36-9f7560407522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03249f43",
   "metadata": {},
   "source": [
    "## RAG with a simple question\n",
    "\n",
    "##### We will ask the question \"In text-to-sql, what are the stages in data generation process?\" <br/>\n",
    "##### We should expect a response from a PDF shown below that includes the three stages shown in picture below.\n",
    "![Image](./image01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12750c99",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199b3bd-3f66-4a29-9929-83cb9efa5723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knowledge Base ID - Choose from different chunking strategies (Fixed, Hierarchical, or Semantic)\n",
    "kb_id = variables[\"kbSemanticChunk\"] \n",
    "\n",
    "# Bedrock Model ARN - Using Amazon Nova Lite for inference\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Number of relevant documents to retrieve for RAG\n",
    "number_of_results = 5\n",
    "\n",
    "# Configuration for text generation - Controls output length, randomness, and diversity\n",
    "generation_configuration = {\n",
    "    'inferenceConfig': {\n",
    "        'textInferenceConfig': {\n",
    "            'maxTokens': 4096,  # Maximum number of tokens in the generated response\n",
    "            'stopSequences': [],  # List of sequences that indicate stopping points\n",
    "            'temperature': 0.2,  # Controls randomness (lower values = more deterministic output)\n",
    "            'topP': 0.5  # Controls diversity of output by considering top P probability mass\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29db82",
   "metadata": {},
   "source": [
    "### Retrieve and Generate with a simple query\n",
    "\n",
    "##### Following query is expected to find the answer from `video_games.csv` which contains sales records of video games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00f7db-57bc-4a91-b9dc-6ba3fef14917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query to search relevant knowledge base documents and generate an answer\n",
    "query = \"What were the third-person view games?\"\n",
    "\n",
    "# Perform retrieval-augmented generation (RAG) using the knowledge base\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": generation_configuration,  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)\n",
    "\n",
    "# Display the full response including citations for retrieved documents\n",
    "print('----------------- Citations ------------------')\n",
    "print(json.dumps(response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7e3a8-5b2f-42de-83dd-e85344ffcdcd",
   "metadata": {},
   "source": [
    "### Comparison between chunking strategies: Fixed vs Semantic\n",
    "\n",
    "##### Now, Let's ask a more nuanced question that needs to extract information from a table in the PDF. Also, let's ask it to do some analysis. <br/>\n",
    "##### We will also compare the response quality when you use fixed size chunking vs Semantic chunking.\n",
    "![image02](image02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e88b9-613a-42cc-ac54-f827902f572f",
   "metadata": {},
   "source": [
    "#### A nuanced query with a Fixed-sized chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb27652-e52e-4dcf-a0e1-6c29f473b52c",
   "metadata": {},
   "source": [
    "##### We will ask question that should answer how net income changed rom 2022 to 2023 to 20234.\n",
    "![image03](image03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd4f32-234b-4719-abd5-f50d91206aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knowledge Base ID - Fixed Chunk.\n",
    "kb_id = variables[\"kbFixedChunk\"] \n",
    "\n",
    "# Bedrock Model ARN - Using Amazon Nova Lite for inference\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Number of relevant documents to retrieve for RAG\n",
    "number_of_results = 5\n",
    "\n",
    "# Configuration for text generation - Controls output length, randomness, and diversity\n",
    "generation_configuration = {\n",
    "    'inferenceConfig': {\n",
    "        'textInferenceConfig': {\n",
    "            'maxTokens': 4096,  # Maximum number of tokens in the generated response\n",
    "            'stopSequences': [],  # List of sequences that indicate stopping points\n",
    "            'temperature': 0.2,  # Controls randomness (lower values = more deterministic output)\n",
    "            'topP': 0.5  # Controls diversity of output by considering top P probability mass\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba9900-a4ea-4f40-95f0-e8ad7b8b621f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query to search relevant knowledge base documents and generate an answer\n",
    "query = \"In CONSOLIDATED STATEMENTS OF CASH FLOWS, How much did net income change in years 2022, 2023, 2024?\"\n",
    "\n",
    "# Perform retrieval-augmented generation (RAG) using the knowledge base\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": generation_configuration,  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51bc3d-39ee-4791-a763-f8285f130e29",
   "metadata": {},
   "source": [
    "#### The response above might not be accurate with what it should be.The accurate response should be:\n",
    "\n",
    "> Year 2022 to Year 2023: \\\\$33,147 increase<br/>\n",
    "Year 2023 to Year 2024: \\\\$28,823 increase \n",
    "\n",
    "#### Now Let's execute the same question while using the KB with Semantic Chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc31f91-f6b0-4b7f-9b20-1732db0c2b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knowledge Base ID - Fixed Chunk.\n",
    "kb_id = variables[\"kbSemanticChunk\"] \n",
    "\n",
    "\n",
    "# Bedrock Model ARN - Using Amazon Nova Lite for inference\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Number of relevant documents to retrieve for RAG\n",
    "number_of_results = 5\n",
    "\n",
    "# Configuration for text generation - Controls output length, randomness, and diversity\n",
    "generation_configuration = {\n",
    "    'inferenceConfig': {\n",
    "        'textInferenceConfig': {\n",
    "            'maxTokens': 4096,  # Maximum number of tokens in the generated response\n",
    "            'stopSequences': [],  # List of sequences that indicate stopping points\n",
    "            'temperature': 0.2,  # Controls randomness (lower values = more deterministic output)\n",
    "            'topP': 0.5  # Controls diversity of output by considering top P probability mass\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab14a1f-ea0c-4361-80b7-5d315beaad88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock Agent Runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the query to search relevant knowledge base documents and generate an answer\n",
    "query = \"In CONSOLIDATED STATEMENTS OF CASH FLOWS, How much did net income change in years 2022, 2023, 2024? Show me how you did the math.\"\n",
    "\n",
    "# Perform retrieval-augmented generation (RAG) using the knowledge base\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": generation_configuration,  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd5820-1e42-463d-8da3-0e59cebf3d27",
   "metadata": {},
   "source": [
    "Compare the above results with the accurate response that should be:\n",
    "> Year 2022 to Year 2023: \\\\$33,147 increase <br/>\n",
    "> Year 2023 to Year 2024: \\\\$28,823 increase\n",
    "\n",
    "As you can see here, Semantic Chunking was able to deliver accurate response as compared to Fixed Size chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c92234",
   "metadata": {},
   "source": [
    "## Improve RAG quality with Enhanced Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab045f5",
   "metadata": {},
   "source": [
    "### Importance of Prompt Engineering\n",
    "Prompt engineering refers to the practice of optimizing textual input to a large language model (LLM) to improve output and receive the responses you want. Prompting helps an LLM perform a wide variety of tasks, including classification, question answering, code generation, creative writing, and more. The quality of prompts that you provide to a LLM can impact the quality of the model's responses. <br/>\n",
    " \n",
    "\n",
    "### Useful techniques to improve prompts for Amazon Nova models\n",
    "Please refer [link](https://docs.aws.amazon.com/nova/latest/userguide/prompting.html) for the best practice of prompt engineering with Amazon Nova models. Fllowings are a few highlights:\n",
    "* Create precise prompts. Provide contextual information, speficy the output format and style, and provide clear prompt sections.\n",
    "* Use system propmts to define how the model will repond.\n",
    "* Give Amazon Nova time to think. For example, add ```\"Think step-by-step.\"``` at the end of your query.\n",
    "* Provide examples.\n",
    "\n",
    "### Tips for using prompts in RAG\n",
    "* Provide Prompt Template: As with other functionalities, enhancing the system prompt can be beneficial. You can define the RAG Systems description in the system prompt, outlining the desired persona and behavior for the model.\n",
    "* Use Model Instructions: Additionally, you can include a dedicated ```\"Model Instructions:\"``` section within the system prompt, where you can provide specific guidelines for the model to follow. For instance, you can list instructions such as: ```In this example session, the model has access to search results and a user's question, its job is to answer the user's question using only information from the search results.```\n",
    "* Avoid Hallucination by restricting the instructions: Bring more focus to instructions by clearly mentioning \"DO NOT USE INFORMATION THAT IS NOT IN SEARCH RESULTS!\" as a model instruction so the answers are grounded in the provided context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73e50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A prompt template with Model Instructions:\n",
    "prompt_template = \"\"\"\n",
    "You are a game sales analyst. Based on the search results, answer questions from users.\n",
    "\n",
    "Model Instructions:\n",
    "- Provide a simple answer first, followed by bullets which support the answer. \n",
    "Bullets include citations from the search results.\n",
    "- When referring specific games, specify the year of publishment and the publisher.\n",
    "- In case the question requires multi-hop reasoning,\n",
    "you should find relevant information from search\n",
    "results and summarize the answer based on relevant\n",
    "information with logical reasoning.\n",
    "- If the search results do not contain information\n",
    "that can answer the question, please state that you\n",
    "could not find an exact answer to the question, and\n",
    "if search results are completely irrelevant, say\n",
    "that you could not find an exact answer, then summarize\n",
    "search results.\n",
    "- DO NOT USE INFORMATION THAT IS NOT IN SEARCH RESULTS!\n",
    "\n",
    "$Query$\n",
    "Resource: $search_results$\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa9d55-474d-403d-ac4f-ecf6a5914886",
   "metadata": {},
   "source": [
    "#### Without a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676924ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"How successful were third-person action games?\"\n",
    "\n",
    "# Perform RAG with/without the prompt template\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": {**generation_configuration\n",
    "                                        },  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d97251-3560-48c0-ad95-13f9ea4ca40a",
   "metadata": {},
   "source": [
    "#### Using a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c4a54-54bc-4aa4-aba2-695d29da0ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"How successful were third-person action games?\"\n",
    "\n",
    "# Perform RAG with/without the prompt template\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query  # User query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,  # ID of the knowledge base used for retrieval\n",
    "            \"modelArn\": model_id,  # Bedrock model ARN for text generation\n",
    "            \"generationConfiguration\": {**generation_configuration\n",
    "                                        , \"promptTemplate\":{\"textPromptTemplate\": prompt_template} # Comment in/out to test the effect of the Prompt Template\n",
    "                                    },  # Model configuration parameters\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results  # Number of relevant documents to fetch\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# Display the generated response\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'], end='\\n' * 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
