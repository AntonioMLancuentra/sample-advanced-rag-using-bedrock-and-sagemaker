{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd35c145-e59c-4c6f-8ce2-3662b5d36151",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with Custom chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c7ebc-5152-4564-bac5-14cb7261aa44",
   "metadata": {},
   "source": [
    "#### Custom Chunking Logic with Lambda Functions in Amazon Bedrock\n",
    "\n",
    "When creating a Knowledge Base (KB) for Amazon Bedrock, you can connect a Lambda function to specify your custom chunking logic. During the ingestion process, if a Lambda function is provided, the Knowledge Base will execute the Lambda function and store the input and output values in the specified intermediate S3 bucket.\n",
    "\n",
    "#### Use Cases for Lambda Functions in KBs\n",
    "\n",
    "- **Custom Chunking Logic:** Lambda functions can be used to implement custom logic for chunking documents during ingestion, enabling more control over how documents are divided into meaningful chunks.\n",
    "- **Chunk-level Metadata Processing:** Lambda functions can also process chunked data, for example, by adding custom metadata at the chunk level, enriching the data for more advanced retrieval or analysis.\n",
    "\n",
    "This allows for more flexibility and tailored handling of document data within the Knowledge Base, making it possible to apply unique chunking strategies and augment the data with specific metadata for improved search and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b6f2e-3e64-4bb0-8f70-d7be425a4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b3661-448e-4717-90cd-e7b56ddf15da",
   "metadata": {},
   "source": [
    "### 0. Create a Lambda function with custom chunking logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24d0d2-ec77-4ef5-b73f-a8877dc21d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for handling files, time, and AWS resources\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "# Create IAM client to interact with AWS IAM service\n",
    "iam = boto3.client(\"iam\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the IAM assume role policy for the Lambda function\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",  # Policy version\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",  # Allow the action\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"lambda.amazonaws.com\"  # Principal entity, Lambda service\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"  # Action Lambda is allowed to perform\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the IAM assume role policy into JSON format\n",
    "assume_role_policy_document_json = json.dumps(assume_role_policy_document)\n",
    "\n",
    "# Create the IAM role for the Lambda function with the assume role policy\n",
    "lambda_iam_role = iam.create_role(\n",
    "    RoleName=f\"advanced-rag-custom-chunk-{variables['regionName']}-role\",  # Define a unique role name based on region\n",
    "    AssumeRolePolicyDocument=assume_role_policy_document_json  # Attach the policy to the role\n",
    ")\n",
    "\n",
    "# Attach an S3 access policy to the newly created IAM role\n",
    "iam.put_role_policy(\n",
    "    RoleName=lambda_iam_role[\"Role\"][\"RoleName\"],  # Specify the role name\n",
    "    PolicyName=\"s3policy\",  # Name of the policy to attach\n",
    "    PolicyDocument=json.dumps(  # Define the policy document granting access to specific S3 resources\n",
    "        {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",  # Allow these actions\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",  # Allow getting objects from S3\n",
    "                        \"s3:ListBucket\",  # Allow listing contents of the S3 bucket\n",
    "                        \"s3:PutObject\"  # Allow putting objects into S3\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        f\"arn:aws:s3:::{variables['s3Bucket']}-custom-chunk\",  # S3 bucket ARN\n",
    "                        f\"arn:aws:s3:::{variables['s3Bucket']}-custom-chunk/*\"  # S3 bucket objects ARN\n",
    "                    ],\n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": f\"{variables['accountNumber']}\"  # Restrict access to the specific AWS account\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prepare the Lambda function code by creating a ZIP file\n",
    "s = BytesIO()  # Create an in-memory binary stream for the zip file\n",
    "z = zipfile.ZipFile(s, 'w')  # Create a ZipFile object for writing\n",
    "z.write(\"lambda_function.py\")  # Add the lambda function code to the zip file\n",
    "z.close()  # Close the ZipFile object\n",
    "zip_content = s.getvalue()  # Get the content of the zip file\n",
    "\n",
    "# Sleep for 10 seconds to ensure resources are available\n",
    "time.sleep(10)\n",
    "\n",
    "# Create the Lambda function using the IAM role, timeout settings, and the ZIP content\n",
    "lambda_function = boto3.client(\"lambda\",\n",
    "                               region_name=variables[\"regionName\"]).create_function(\n",
    "    FunctionName=\"advanced-rag-custom-chunk\",  # Define the Lambda function name\n",
    "    Runtime='python3.12',  # Specify the runtime environment (Python 3.12)\n",
    "    Timeout=60,  # Set the timeout for the Lambda function (in seconds)\n",
    "    Role=lambda_iam_role['Role']['Arn'],  # IAM role ARN for Lambda function execution\n",
    "    Code={'ZipFile': zip_content},  # Provide the zipped Lambda function code\n",
    "    Handler='lambda_function.lambda_handler'  # Define the handler function in the code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325eb5bd-f3a6-4144-835d-5b6db60fd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client to interact with the AWS S3 service in the specified region\n",
    "s3 = boto3.client(\"s3\", region_name=variables[\"regionName\"])\n",
    "\n",
    "try:\n",
    "    # Check if the bucket already exists by sending a HEAD request to S3\n",
    "    s3.head_bucket(Bucket=variables[\"s3Bucket\"]+\"-custom-chunk\")\n",
    "    # If the bucket exists, print a message\n",
    "    print(f\"Bucket '{variables['s3Bucket']}' already exists.\")\n",
    "except:\n",
    "    # If the bucket does not exist, create a new one\n",
    "    s3.create_bucket(Bucket=variables[\"s3Bucket\"]+\"-custom-chunk\", CreateBucketConfiguration={\n",
    "        'LocationConstraint': variables[\"regionName\"]})  # Specify the region for the new bucket\n",
    "    # Print a message indicating the bucket has been created\n",
    "    print(f\"Bucket '{variables['s3Bucket']}-custom-chunk' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446f270-acf7-4278-a664-643169ee2646",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a398d-a1b2-4552-9669-5316f303b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function definition\n",
    "from retrying import retry  # Import retrying module to add retry logic\n",
    "import boto3  # Import boto3 for AWS SDK to interact with AWS services\n",
    "\n",
    "# Create a Bedrock agent client to interact with Amazon Bedrock service\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Retry logic added to the function, which will retry the function 3 times with a random wait time between 1-2 seconds\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=3)\n",
    "def create_knowledge_base_func(name, description, chunking_type):\n",
    "    # Define the embedding model ARN that will be used by Bedrock for embedding ingested documents\n",
    "    embedding_model_arn = f\"arn:aws:bedrock:{variables['regionName']}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "    \n",
    "    # Define OpenSearch Serverless configuration that includes the collection and vector index names\n",
    "    opensearch_serverless_configuration = {\n",
    "            \"collectionArn\": variables[\"collectionArn\"],  # ARN of the OpenSearch collection\n",
    "            \"vectorIndexName\": variables[\"vectorIndexName\"] + chunking_type,  # Name of the vector index\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",  # Field name for the vector\n",
    "                \"textField\": \"text\",      # Field name for the text\n",
    "                \"metadataField\": \"text-metadata\"  # Field name for the metadata\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Print the OpenSearch configuration for debugging purposes\n",
    "    print(opensearch_serverless_configuration)\n",
    "    \n",
    "    # Call the Bedrock API to create the knowledge base with the specified configurations\n",
    "    create_kb_response = bedrock_agent.create_knowledge_base(\n",
    "        name=name,  # Name of the knowledge base\n",
    "        description=description,  # Description of the knowledge base\n",
    "        roleArn=variables[\"bedrockExecutionRoleArn\"],  # ARN of the IAM role that Bedrock will use for execution\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",  # Type of the knowledge base (VECTOR in this case)\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embedding_model_arn  # ARN of the embedding model used for the knowledge base\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",  # Type of the storage (using OpenSearch Serverless)\n",
    "            \"opensearchServerlessConfiguration\": opensearch_serverless_configuration  # OpenSearch configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Return the created knowledge base details\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36ef26-2058-454e-b4ba-1804e086fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3  # Import boto3 to interact with AWS services\n",
    "\n",
    "# Create a knowledge base using the previously defined function\n",
    "kb = create_knowledge_base_func(\n",
    "    name=\"advanced-rag-workshop-custom-chunking\",  # Name of the knowledge base\n",
    "    description=\"Knowledge base using Amazon OpenSearch Service as a vector store\",  # Description of the knowledge base\n",
    "    chunking_type=\"custom\"  # Custom chunking type for the knowledge base\n",
    ")\n",
    "\n",
    "# Get the details of the created knowledge base using the knowledgeBaseId\n",
    "get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "\n",
    "# Write the variables and knowledge base ID to a JSON file for future use\n",
    "with open(\"variables.json\", \"w\") as f:\n",
    "    # Save the variables along with the new knowledge base ID to the file\n",
    "    json.dump({**variables, \"kbCustomChunk\": kb['knowledgeBaseId']}, f)\n",
    "\n",
    "# Print the response from OpenSearch for debugging or logging purposes\n",
    "print(f'OpenSearch Knowledge Response: {get_kb_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684676c-0c32-47aa-9d48-d2c38b5a6206",
   "metadata": {},
   "source": [
    "### 2. Create Datasources for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081015d-a84e-4481-a6fd-381d86417450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Import time to manage sleep intervals between actions\n",
    "\n",
    "# Ingest strategy: Defines how to ingest data from the data source into the OpenSearch knowledge base\n",
    "customTransformationConfiguration = {\n",
    "    \"intermediateStorage\": {\n",
    "        # Specifies the S3 location where intermediate data will be stored\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{variables['s3Bucket']}-custom-chunk/\"  # Using the custom chunk bucket in S3\n",
    "        }\n",
    "    },\n",
    "    \"transformations\": [\n",
    "        {\n",
    "            \"transformationFunction\": {\n",
    "                # Specifies the Lambda function to apply for data transformation\n",
    "                \"transformationLambdaConfiguration\": {\n",
    "                    \"lambdaArn\": f\"arn:aws:lambda:{variables['regionName']}:{variables['accountNumber']}:function:advanced-rag-custom-chunk\"  # ARN of the Lambda function\n",
    "                }\n",
    "            },\n",
    "            \"stepToApply\": \"POST_CHUNKING\"  # Defines when to apply the transformation function (after chunking)\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Defines the S3 data source configuration for ingesting documents into OpenSearch\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{variables['s3Bucket']}\",  # ARN for the S3 bucket\n",
    "    # \"inclusionPrefixes\": [\"shareholder_letters\"]  # Optional: specify a prefix for the S3 objects to ingest\n",
    "}\n",
    "\n",
    "# Check if 'ds_custom_chunk' is already defined and delete if necessary\n",
    "if 'ds_custom_chunk' in locals():\n",
    "    try:\n",
    "        # If the data source already exists, delete it to avoid conflicts\n",
    "        bedrock_agent.delete_data_source(\n",
    "            knowledgeBaseId = kb['knowledgeBaseId'],  # Knowledge base ID\n",
    "            dataSourceId = ds_custom_chunk[\"dataSourceId\"],  # Data source ID to delete\n",
    "        )\n",
    "        time.sleep(15)  # Wait for 15 seconds before proceeding\n",
    "    except Exception as e:\n",
    "        print(e)  # Print the error if any\n",
    "        pass  # Continue without any action if there is an exception\n",
    "\n",
    "# Create a new data source for the knowledge base\n",
    "create_ds_response = bedrock_agent.create_data_source(\n",
    "    name = f'advanced-rag-example',  # Name of the new data source\n",
    "    description = \"A data source for Advanced RAG workshop\",  # Description of the data source\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],  # Knowledge base ID where the data source is added\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",  # Specifies the data source type (S3 in this case)\n",
    "        \"s3Configuration\": s3Configuration  # S3 configuration defined earlier\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        # Defines how vector ingestion is handled for this data source\n",
    "        \"chunkingConfiguration\": {\"chunkingStrategy\": \"NONE\"},  # No chunking strategy (all content as a single chunk)\n",
    "        \"customTransformationConfiguration\": customTransformationConfiguration  # Custom transformation configuration\n",
    "    }\n",
    ")\n",
    "\n",
    "# Store the created data source information\n",
    "ds_custom_chunk = create_ds_response[\"dataSource\"]\n",
    "\n",
    "# Return the created data source information\n",
    "ds_custom_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee4ce6-10ee-4c4b-ae9a-8d16be4ed48b",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769885e3",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in the OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d003980-60ba-4b7e-94c2-1bc99c0b63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Import time to manage delays during job status checking\n",
    "\n",
    "ingest_jobs = []  # Initialize a list to track ingestion jobs\n",
    "\n",
    "# Start an ingestion job\n",
    "try:\n",
    "    # Start the ingestion job for the data source into the knowledge base\n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId = kb['knowledgeBaseId'],  # Knowledge base ID\n",
    "        dataSourceId = ds_custom_chunk[\"dataSourceId\"]  # Data source ID for ingestion\n",
    "    )\n",
    "    \n",
    "    # Get the job details from the response\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(f\"ingestion job started successfully\\n\")  # Print message indicating job start\n",
    "\n",
    "    # Check the status of the job until it completes\n",
    "    while(job['status'] != 'COMPLETE'):  # Continue checking until job status is 'COMPLETE'\n",
    "        # Retrieve the current status of the ingestion job\n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId = kb['knowledgeBaseId'],  # Knowledge base ID\n",
    "            dataSourceId = ds_custom_chunk[\"dataSourceId\"],  # Data source ID\n",
    "            ingestionJobId = job[\"ingestionJobId\"]  # Job ID to track\n",
    "        )\n",
    "        \n",
    "        # Update the job status from the response\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "\n",
    "    time.sleep(15)  # Wait for 15 seconds before proceeding to ensure job completion\n",
    "    print(f\"job completed successfully\\n\")  # Print message indicating successful job completion\n",
    "\n",
    "except Exception as e:\n",
    "    # If there is any error, print an error message\n",
    "    print(f\"Couldn't start job.\\n\")\n",
    "    print(e)  # Print the exception error message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1edb27-a7f5-4ab4-8770-2de83134b430",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7fad3-3114-4010-9870-5f52e5fb5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock agent runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Query for relevant documents\n",
    "query = \"What are three hree sub-tasks in question answering over knowledge bases?\"  # Define the query to be searched\n",
    "\n",
    "# Retrieve relevant documents based on the query from the knowledge base\n",
    "relevant_documents_os = bedrock_agent_runtime.retrieve(\n",
    "    retrievalQuery={\n",
    "        'text': query  # Specify the query text to search for relevant documents\n",
    "    },\n",
    "    knowledgeBaseId=kb['knowledgeBaseId'],  # Provide the knowledge base ID to search in\n",
    "    retrievalConfiguration={\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3  # Limit the results to top 3 documents closely matching the query\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Return the relevant documents fetched\n",
    "relevant_documents_os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e804b",
   "metadata": {},
   "source": [
    "> **Note**: After creating the knowledge base, you can explore its details and settings in the Amazon Bedrock console. This gives you a more visual interface to understand how the knowledge base is structured.\n",
    "> \n",
    "> **[➡️ View your Knowledge Bases in the AWS Console](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/knowledge-bases)**\n",
    ">\n",
    "> In the console, you can:\n",
    "> - See all your knowledge bases in one place\n",
    "> - View ingestion status and statistics\n",
    "> - Test queries through the built-in chat interface\n",
    "> - Modify settings and configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
