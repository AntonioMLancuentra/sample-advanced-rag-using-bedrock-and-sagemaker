{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd35c145-e59c-4c6f-8ce2-3662b5d36151",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with Custom chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c7ebc-5152-4564-bac5-14cb7261aa44",
   "metadata": {},
   "source": [
    "#### Custom Chunking Logic with Lambda Functions in Amazon Bedrock\n",
    "\n",
    "When creating a Knowledge Base (KB) for Amazon Bedrock, you can connect a Lambda function to specify your custom chunking logic. During the ingestion process, if a Lambda function is provided, the Knowledge Base will execute the Lambda function and store the input and output values in the specified intermediate S3 bucket.\n",
    "\n",
    "#### Use Cases for Lambda Functions in KBs\n",
    "\n",
    "- **Custom Chunking Logic:** Lambda functions can be used to implement custom logic for chunking documents during ingestion, enabling more control over how documents are divided into meaningful chunks.\n",
    "- **Chunk-level Metadata Processing:** Lambda functions can also process chunked data, for example, by adding custom metadata at the chunk level, enriching the data for more advanced retrieval or analysis.\n",
    "\n",
    "This allows for more flexibility and tailored handling of document data within the Knowledge Base, making it possible to apply unique chunking strategies and augment the data with specific metadata for improved search and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b6f2e-3e64-4bb0-8f70-d7be425a4a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b3661-448e-4717-90cd-e7b56ddf15da",
   "metadata": {},
   "source": [
    "### 0. Create a Lambda function with custom chunking logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24d0d2-ec77-4ef5-b73f-a8877dc21d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import botocore\n",
    "\n",
    "# Create IAM client to interact with AWS IAM service\n",
    "iam = boto3.client(\"iam\", region_name=variables[\"regionName\"])\n",
    "lambda_client = boto3.client(\"lambda\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Define the role name\n",
    "role_name = f\"advanced-rag-custom-chunk-{variables['regionName']}-role\"\n",
    "function_name = \"advanced-rag-custom-chunk\"\n",
    "\n",
    "# Try to get the IAM role if it exists\n",
    "try:\n",
    "    # Check if the role already exists\n",
    "    get_role_response = iam.get_role(RoleName=role_name)\n",
    "    lambda_iam_role = get_role_response  # Store the entire response\n",
    "    print(f\"IAM role '{role_name}' already exists. Using the existing role.\")\n",
    "except iam.exceptions.NoSuchEntityException:\n",
    "    # Define the IAM assume role policy for the Lambda function\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Convert the IAM assume role policy into JSON format\n",
    "    assume_role_policy_document_json = json.dumps(assume_role_policy_document)\n",
    "    \n",
    "    # Create the IAM role for the Lambda function with the assume role policy\n",
    "    lambda_iam_role = iam.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=assume_role_policy_document_json\n",
    "    )\n",
    "    print(f\"Created new IAM role: {role_name}\")\n",
    "\n",
    "# Always put the policy (it will update if it exists or create if it doesn't)\n",
    "iam.put_role_policy(\n",
    "    RoleName=role_name,  # Use role name directly instead of lambda_iam_role[\"Role\"][\"RoleName\"]\n",
    "    PolicyName=\"s3policy\",\n",
    "    PolicyDocument=json.dumps(\n",
    "        {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": [\n",
    "                        \"s3:GetObject\",\n",
    "                        \"s3:ListBucket\", \n",
    "                        \"s3:PutObject\"\n",
    "                    ],\n",
    "                    \"Resource\": [\n",
    "                        f\"arn:aws:s3:::{variables['s3Bucket']}-custom-chunk\",\n",
    "                        f\"arn:aws:s3:::{variables['s3Bucket']}-custom-chunk/*\"\n",
    "                    ],\n",
    "                    \"Condition\": {\n",
    "                        \"StringEquals\": {\n",
    "                            \"aws:ResourceAccount\": f\"{variables['accountNumber']}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prepare the Lambda function code by creating a ZIP file\n",
    "s = BytesIO()\n",
    "z = zipfile.ZipFile(s, 'w')\n",
    "z.write(\"lambda_function.py\")\n",
    "z.close()\n",
    "zip_content = s.getvalue()\n",
    "\n",
    "# Sleep for 10 seconds to ensure resources are available\n",
    "time.sleep(10)\n",
    "\n",
    "# Get the role ARN\n",
    "role_arn = lambda_iam_role[\"Role\"][\"Arn\"]\n",
    "\n",
    "# Check if the Lambda function already exists\n",
    "try:\n",
    "    lambda_client.get_function(FunctionName=function_name)\n",
    "    print(f\"Lambda function '{function_name}' already exists. Updating code...\")\n",
    "    \n",
    "    # Update existing function code\n",
    "    lambda_function = lambda_client.update_function_code(\n",
    "        FunctionName=function_name,\n",
    "        ZipFile=zip_content\n",
    "    )\n",
    "    print(\"Lambda function code updated successfully\")\n",
    "except lambda_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"Creating new Lambda function: {function_name}\")\n",
    "    \n",
    "    # Create the Lambda function\n",
    "    lambda_function = lambda_client.create_function(\n",
    "        FunctionName=function_name,\n",
    "        Runtime='python3.12',\n",
    "        Timeout=60,\n",
    "        Role=role_arn,\n",
    "        Code={'ZipFile': zip_content},\n",
    "        Handler='lambda_function.lambda_handler'\n",
    "    )\n",
    "    print(\"Lambda function created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325eb5bd-f3a6-4144-835d-5b6db60fd516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an S3 client to interact with the AWS S3 service in the specified region\n",
    "s3 = boto3.client(\"s3\", region_name=variables[\"regionName\"])\n",
    "\n",
    "try:\n",
    "    # Check if the bucket already exists by sending a HEAD request to S3\n",
    "    s3.head_bucket(Bucket=variables[\"s3Bucket\"]+\"-custom-chunk\")\n",
    "    # If the bucket exists, print a message\n",
    "    print(f\"Bucket '{variables['s3Bucket']}' already exists.\")\n",
    "except:\n",
    "    # If the bucket does not exist, create a new one\n",
    "    s3.create_bucket(Bucket=variables[\"s3Bucket\"]+\"-custom-chunk\", CreateBucketConfiguration={\n",
    "        'LocationConstraint': variables[\"regionName\"]})  # Specify the region for the new bucket\n",
    "    # Print a message indicating the bucket has been created\n",
    "    print(f\"Bucket '{variables['s3Bucket']}-custom-chunk' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446f270-acf7-4278-a664-643169ee2646",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a398d-a1b2-4552-9669-5316f303b6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function definition\n",
    "from retrying import retry  # Import retrying module to add retry logic\n",
    "import boto3  # Import boto3 for AWS SDK to interact with AWS services\n",
    "\n",
    "# Create a Bedrock agent client to interact with Amazon Bedrock service\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Retry logic added to the function, which will retry the function 3 times with a random wait time between 1-2 seconds\n",
    "@retry(wait_random_min=1000, wait_random_max=2000, stop_max_attempt_number=3)\n",
    "def create_knowledge_base_func(name, description, chunking_type):\n",
    "    # Define the embedding model ARN that will be used by Bedrock for embedding ingested documents\n",
    "    embedding_model_arn = f\"arn:aws:bedrock:{variables['regionName']}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "    \n",
    "    # Define OpenSearch Serverless configuration that includes the collection and vector index names\n",
    "    opensearch_serverless_configuration = {\n",
    "            \"collectionArn\": variables[\"collectionArn\"],  # ARN of the OpenSearch collection\n",
    "            \"vectorIndexName\": variables[\"vectorIndexName\"] + chunking_type,  # Name of the vector index\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",  # Field name for the vector\n",
    "                \"textField\": \"text\",      # Field name for the text\n",
    "                \"metadataField\": \"text-metadata\"  # Field name for the metadata\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Print the OpenSearch configuration for debugging purposes\n",
    "    print(opensearch_serverless_configuration)\n",
    "    \n",
    "    # Call the Bedrock API to create the knowledge base with the specified configurations\n",
    "    create_kb_response = bedrock_agent.create_knowledge_base(\n",
    "        name=name,  # Name of the knowledge base\n",
    "        description=description,  # Description of the knowledge base\n",
    "        roleArn=variables[\"bedrockExecutionRoleArn\"],  # ARN of the IAM role that Bedrock will use for execution\n",
    "        knowledgeBaseConfiguration={\n",
    "            \"type\": \"VECTOR\",  # Type of the knowledge base (VECTOR in this case)\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embedding_model_arn  # ARN of the embedding model used for the knowledge base\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration={\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",  # Type of the storage (using OpenSearch Serverless)\n",
    "            \"opensearchServerlessConfiguration\": opensearch_serverless_configuration  # OpenSearch configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Return the created knowledge base details\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36ef26-2058-454e-b4ba-1804e086fcd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Create a knowledge base using the predefined function\n",
    "    kb = create_knowledge_base_func(\n",
    "        name=\"advanced-rag-workshop-custom-chunking\",\n",
    "        description=\"Knowledge base using Amazon OpenSearch Service as a vector store\",\n",
    "        chunking_type=\"custom\"\n",
    "    )\n",
    "\n",
    "    # Retrieve details of the newly created knowledge base\n",
    "    get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "\n",
    "    # Update the variables dictionary with the new knowledge base ID\n",
    "    variables[\"kbCustomChunk\"] = kb['knowledgeBaseId']\n",
    "\n",
    "    # Save updated variables to a JSON file, handling datetime serialization\n",
    "    with open(\"variables.json\", \"w\") as f:\n",
    "        json.dump(variables, f, indent=4, default=str)  # Convert datetime to string\n",
    "\n",
    "    # Print the retrieved knowledge base response in a readable format\n",
    "    print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "    \n",
    "except Exception as e:\n",
    "    # Check if error message indicates the knowledge base already exists\n",
    "    error_message = str(e).lower()\n",
    "    if any(phrase in error_message for phrase in [\"already exist\", \"duplicate\", \"already been created\"]):\n",
    "        print(\"Knowledge Base already exists. Retrieving its ID...\")\n",
    "        \n",
    "        # List all knowledge bases to find the one that already exists\n",
    "        list_kb_response = bedrock_agent.list_knowledge_bases()\n",
    "        \n",
    "        # Look for a knowledge base with the desired name\n",
    "        for existing_kb in list_kb_response.get('knowledgeBaseSummaries', []):\n",
    "            if existing_kb['name'] == \"advanced-rag-workshop-custom-chunking\":\n",
    "                kb_id = existing_kb['knowledgeBaseId']\n",
    "                print(f\"Found existing knowledge base with ID: {kb_id}\")\n",
    "                \n",
    "                # Get the details of the existing knowledge base\n",
    "                get_kb_response = bedrock_agent.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "                \n",
    "                # Read existing variables to preserve other fields\n",
    "                try:\n",
    "                    # Read existing variables\n",
    "                    with open(\"variables.json\", \"r\") as f:\n",
    "                        existing_variables = json.load(f)\n",
    "                except (FileNotFoundError, json.JSONDecodeError):\n",
    "                    # If file doesn't exist or is invalid JSON\n",
    "                    existing_variables = {}\n",
    "                \n",
    "                # Update only the custom chunking value\n",
    "                existing_variables[\"kbCustomChunk\"] = kb_id\n",
    "                                \n",
    "                # Write back all variables\n",
    "                with open(\"variables.json\", \"w\") as f:\n",
    "                    json.dump(existing_variables, f, indent=4, default=str)\n",
    "                \n",
    "                # Print the retrieved knowledge base response\n",
    "                print(f'OpenSearch Knowledge Response: {json.dumps(get_kb_response, indent=4, default=str)}')\n",
    "                break        \n",
    "        else:\n",
    "            print(\"Could not find a knowledge base with the specified name.\")\n",
    "    else:\n",
    "        # If it's a different error, re-raise it\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684676c-0c32-47aa-9d48-d2c38b5a6206",
   "metadata": {},
   "source": [
    "### 2. Create Datasources for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081015d-a84e-4481-a6fd-381d86417450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create clients\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\", region_name=variables[\"regionName\"])\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Load variables to get the correct knowledge base ID\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "# Use the correct knowledge base ID - kbCustomChunk\n",
    "kb_id = variables.get(\"kbCustomChunk\")\n",
    "\n",
    "if not kb_id:\n",
    "    print(\"Error: No knowledge base ID found for custom chunking!\")\n",
    "    raise ValueError(\"Knowledge base ID missing\")\n",
    "\n",
    "print(f\"Using knowledge base ID: {kb_id}\")\n",
    "\n",
    "# Define custom transformation configuration\n",
    "custom_transformation_configuration = {\n",
    "    \"intermediateStorage\": {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{variables['s3Bucket']}-custom-chunk/\"\n",
    "        }\n",
    "    },\n",
    "    \"transformations\": [\n",
    "        {\n",
    "            \"transformationFunction\": {\n",
    "                \"transformationLambdaConfiguration\": {\n",
    "                    \"lambdaArn\": f\"arn:aws:lambda:{variables['regionName']}:{variables['accountNumber']}:function:advanced-rag-custom-chunk\"\n",
    "                }\n",
    "            },\n",
    "            \"stepToApply\": \"POST_CHUNKING\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define S3 data source configuration with data prefix\n",
    "s3_configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{variables['s3Bucket']}\",\n",
    "    \"inclusionPrefixes\": [\"data\"]  # Only include objects with the \"data\" prefix\n",
    "}\n",
    "\n",
    "data_source_name = \"advanced-rag-example\"\n",
    "\n",
    "# Check if data source already exists\n",
    "try:\n",
    "    print(f\"Checking for existing data sources in knowledge base {kb_id}...\")\n",
    "    list_ds_response = bedrock_agent.list_data_sources(knowledgeBaseId=kb_id)\n",
    "    \n",
    "    existing_ds = None\n",
    "    for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "        if ds['name'] == data_source_name:\n",
    "            existing_ds = ds\n",
    "            break\n",
    "    \n",
    "    if existing_ds:\n",
    "        print(f\"Found existing data source '{data_source_name}' with ID {existing_ds['dataSourceId']}. Deleting it...\")\n",
    "        bedrock_agent.delete_data_source(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=existing_ds[\"dataSourceId\"]\n",
    "        )\n",
    "        print(\"Waiting for data source deletion to complete...\")\n",
    "        time.sleep(20)  # Increased wait time\n",
    "        print(\"Data source deleted.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error while checking or deleting data source: {e}\")\n",
    "\n",
    "# Create the new data source\n",
    "try:\n",
    "    print(f\"Creating new data source '{data_source_name}' with custom chunking for knowledge base {kb_id}...\")\n",
    "    create_ds_response = bedrock_agent.create_data_source(\n",
    "        name=data_source_name,\n",
    "        description=\"A data source for Advanced RAG workshop\",\n",
    "        knowledgeBaseId=kb_id,\n",
    "        dataSourceConfiguration={\n",
    "            \"type\": \"S3\",\n",
    "            \"s3Configuration\": s3_configuration\n",
    "        },\n",
    "        vectorIngestionConfiguration={\n",
    "            \"chunkingConfiguration\": {\"chunkingStrategy\": \"NONE\"},\n",
    "            \"customTransformationConfiguration\": custom_transformation_configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ds_custom_chunk = create_ds_response[\"dataSource\"]\n",
    "    ds_id = ds_custom_chunk[\"dataSourceId\"]\n",
    "    print(f\"Custom chunking data source created successfully with ID: {ds_id}\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response.get('Error', {}).get('Code', '')\n",
    "    if error_code == 'ConflictException':\n",
    "        print(f\"Data source '{data_source_name}' already exists. Retrieving it...\")\n",
    "        list_ds_response = bedrock_agent.list_data_sources(knowledgeBaseId=kb_id)\n",
    "        for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "            if ds['name'] == data_source_name:\n",
    "                ds_custom_chunk = ds\n",
    "                ds_id = ds['dataSourceId']\n",
    "                print(f\"Retrieved existing data source with ID: {ds_id}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Error creating data source: {e}\")\n",
    "        raise\n",
    "\n",
    "# Start an ingestion job\n",
    "try:\n",
    "    print(f\"Starting ingestion job for data source {ds_id}...\")\n",
    "    \n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        dataSourceId=ds_id\n",
    "    )\n",
    "    \n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(f\"Ingestion job started with ID: {job['ingestionJobId']}\")\n",
    "\n",
    "    print(\"Monitoring ingestion job status...\")\n",
    "    while job['status'] not in ['COMPLETE', 'FAILED', 'STOPPED']:\n",
    "        print(f\"Current status: {job['status']} - waiting...\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=ds_id,\n",
    "            ingestionJobId=job[\"ingestionJobId\"]\n",
    "        )\n",
    "        \n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "\n",
    "    if job['status'] == 'COMPLETE':\n",
    "        print(f\"Ingestion job completed successfully!\")\n",
    "        print(f\"Statistics: {job.get('statistics', 'No statistics available')}\")\n",
    "    else:\n",
    "        print(f\"Ingestion job ended with status: {job['status']}\")\n",
    "        print(f\"Failure reason: {job.get('failureReasons', 'No failure reason provided')}\")\n",
    "\n",
    "    # Test the retrieval after successful ingestion\n",
    "    print(\"\\nTesting retrieval...\")\n",
    "    query = \"What were net incomes of Amazon in 2022, 2023 and 2024?\"\n",
    "    \n",
    "    relevant_documents = bedrock_agent_runtime.retrieve(\n",
    "        retrievalQuery={'text': query},\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {'numberOfResults': 3}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(json.dumps([i[\"content\"][\"text\"] for i in relevant_documents[\"retrievalResults\"]], indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with ingestion job: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee4ce6-10ee-4c4b-ae9a-8d16be4ed48b",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769885e3",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in the OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d003980-60ba-4b7e-94c2-1bc99c0b63ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# First, get the knowledge base ID (since kb is not defined)\n",
    "print(\"Retrieving knowledge base ID...\")\n",
    "\n",
    "# Try to load from variables.json\n",
    "try:\n",
    "    with open(\"variables.json\", \"r\") as f:\n",
    "        variables = json.load(f)\n",
    "    \n",
    "    # Check if a knowledge base ID is already defined in variables\n",
    "    kb_id = variables.get(\"knowledgeBaseId\")\n",
    "    if kb_id:\n",
    "        print(f\"Using knowledge base ID from variables: {kb_id}\")\n",
    "    else:\n",
    "        # If not found in variables, list all knowledge bases and use the first one\n",
    "        list_kb_response = bedrock_agent.list_knowledge_bases()\n",
    "        if list_kb_response.get('knowledgeBaseSummaries'):\n",
    "            kb_id = list_kb_response['knowledgeBaseSummaries'][0]['knowledgeBaseId']\n",
    "            print(f\"Using first available knowledge base ID: {kb_id}\")\n",
    "        else:\n",
    "            print(\"No knowledge bases found!\")\n",
    "            raise ValueError(\"No knowledge base ID available\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving knowledge base ID: {e}\")\n",
    "    raise\n",
    "\n",
    "# Now make sure we have a valid data source ID\n",
    "if 'ds_custom_chunk' not in locals():\n",
    "    print(\"Looking for the data source...\")\n",
    "    try:\n",
    "        # List all data sources for the knowledge base\n",
    "        list_ds_response = bedrock_agent.list_data_sources(knowledgeBaseId=kb_id)\n",
    "        \n",
    "        for ds in list_ds_response.get('dataSourceSummaries', []):\n",
    "            if ds['name'] == \"advanced-rag-example\":\n",
    "                ds_custom_chunk = ds\n",
    "                print(f\"Found data source: {ds['dataSourceId']}\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Data source 'advanced-rag-example' not found!\")\n",
    "            raise ValueError(\"Required data source not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error looking for data source: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize a list to track ingestion jobs\n",
    "ingest_jobs = []  \n",
    "\n",
    "# Start an ingestion job\n",
    "try:\n",
    "    print(f\"Starting ingestion job for data source {ds_custom_chunk['dataSourceId']}...\")\n",
    "    \n",
    "    # Start the ingestion job\n",
    "    start_job_response = bedrock_agent.start_ingestion_job(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        dataSourceId=ds_custom_chunk[\"dataSourceId\"]\n",
    "    )\n",
    "    \n",
    "    # Get the job details from the response\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print(f\"Ingestion job started successfully\\n\")\n",
    "\n",
    "    # Check the status of the job until it completes\n",
    "    while job['status'] not in ['COMPLETE', 'FAILED', 'STOPPED']:\n",
    "        print(\"running...\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "        # Retrieve the current status of the ingestion job\n",
    "        get_job_response = bedrock_agent.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            dataSourceId=ds_custom_chunk[\"dataSourceId\"],\n",
    "            ingestionJobId=job[\"ingestionJobId\"]\n",
    "        )\n",
    "        \n",
    "        # Update the job status from the response\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "\n",
    "    if job['status'] == 'COMPLETE':\n",
    "        print(f\"Job completed successfully\\n\")\n",
    "    else:\n",
    "        print(f\"Job ended with status: {job['status']}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Couldn't start job.\\n\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1edb27-a7f5-4ab4-8770-2de83134b430",
   "metadata": {},
   "source": [
    "### 4. Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7fad3-3114-4010-9870-5f52e5fb5e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock agent runtime client\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "kb_id = variables.get(\"kbCustomChunk\")\n",
    "\n",
    "# Query for relevant documents\n",
    "query = \"What were net incomes of Amazon in 2022, 2023 and 2024?\" \n",
    "\n",
    "# Retrieve relevant documents based on the query from the knowledge base\n",
    "relevant_documents_os = bedrock_agent_runtime.retrieve(\n",
    "    retrievalQuery={\n",
    "        'text': query  # Specify the query text to search for relevant documents\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,  # Provide the knowledge base ID to search in\n",
    "    retrievalConfiguration={\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3  # Limit the results to top 3 documents closely matching the query\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Return the relevant documents fetched\n",
    "print(json.dumps([i[\"content\"][\"text\"] for i in relevant_documents_os[\"retrievalResults\"]], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e804b",
   "metadata": {},
   "source": [
    "> **Note**: After creating the knowledge base, you can explore its details and settings in the Amazon Bedrock console. This gives you a more visual interface to understand how the knowledge base is structured.\n",
    "> \n",
    "> **[➡️ View your Knowledge Bases in the AWS Console](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/knowledge-bases)**\n",
    ">\n",
    "> In the console, you can:\n",
    "> - See all your knowledge bases in one place\n",
    "> - View ingestion status and statistics\n",
    "> - Test queries through the built-in chat interface\n",
    "> - Modify settings and configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
