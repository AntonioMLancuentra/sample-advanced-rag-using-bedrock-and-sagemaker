{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61938163-1eb5-42b8-96d7-7937cb1dcfda",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with SageMaker Endpoint LLM\n",
    "\n",
    "## Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using a SageMaker-hosted large language model (LLM). We will retrieve relevant documents from a knowledge base and use the LLM to generate responses based on the retrieved information.  \n",
    "\n",
    "## Key Steps:  \n",
    "- Configure and query a knowledge base for relevant documents.  \n",
    "- Use a SageMaker-hosted LLM to generate contextual responses.  \n",
    "- Optimize retrieval and generation parameters for improved accuracy.  \n",
    "\n",
    "By the end of this notebook, you'll understand how to integrate SageMaker-hosted models into a RAG pipeline to enhance answer generation with domain-specific knowledge.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dad9fe",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d738b9a-e03a-48eb-a296-37dba1bfc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 langchain-aws 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb976f-39c7-442a-90a9-8261683f0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4588ac-1fb5-4e88-aa00-17bf4fbe7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Configuration  \n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\"  # Choose a language model (e.g., LLaMA 3, DeepSeek, etc.)\n",
    "instance_type = \"ml.g5.4xlarge\"  # Define the SageMaker instance type for model inference\n",
    "\n",
    "# Knowledge Base Selection  \n",
    "kb_id = variables[\"kbFixedChunk\"]  # Options: \"kbFixedChunk\", \"kbHierarchicalChunk\", \"kbSemanticChunk\"\n",
    "\n",
    "# Retrieval-Augmented Generation (RAG) Configuration  \n",
    "number_of_results = 3  # Number of relevant documents to retrieve  \n",
    "generation_configuration = {\n",
    "    \"temperature\": 0,  # Lower temperature for more deterministic responses  \n",
    "    \"top_k\": 10,  # Consider top 10 tokens at each generation step  \n",
    "    \"max_new_tokens\": 5000,  # Maximum number of tokens to generate  \n",
    "    \"stop\": \"<|eot_id|>\"  # Stop sequence to end the response generation  \n",
    "}\n",
    "\n",
    "# User Query  \n",
    "query = \"What are three hree sub-tasks in question answering over knowledge bases?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaef16a",
   "metadata": {},
   "source": [
    "> **Note**: The model deployment process for the SageMaker endpoint will take approximately 10-15 minutes to complete. During this time, the system is:\n",
    "> 1. Provisioning the required compute resources (GPU instances)\n",
    "> 2. Downloading and installing the model artifacts\n",
    "> 3. Configuring the inference environment\n",
    "> 4. Setting up auto-scaling and monitoring for the endpoint\n",
    ">\n",
    "> No further action is needed during this time. The cell will continue to execute until the endpoint is fully deployed and ready for inference. This is a one-time setup that will be used throughout the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b20928-cd3a-4243-9f0c-0fb164cd0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# Load the JumpStart model with the specified model ID and instance type  \n",
    "llm_model = JumpStartModel(model_id=model_id, instance_type=instance_type)\n",
    "\n",
    "# Deploy the model as a SageMaker endpoint  \n",
    "llm_endpoint = llm_model.deploy(\n",
    "    accept_eula=True,  # Accept the model's End User License Agreement (EULA)  \n",
    "    initial_instance_count=1,  # Number of instances for hosting the model  \n",
    "    endpoint_name=\"llm-inference-advanced-rag\"  # Custom name for the deployed endpoint  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09242d1-4586-43e3-83ef-1243d4cd7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SageMaker endpoint name to the variables JSON file  \n",
    "with open(\"variables.json\", \"w\") as f:\n",
    "    json.dump({**variables, \"sagemakerLLMEndpoint\": llm_endpoint.endpoint_name}, f)\n",
    "\n",
    "# Print or return the deployed SageMaker endpoint name  \n",
    "llm_endpoint.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e138a0-75e5-4b02-ac48-e875a18ce5ca",
   "metadata": {},
   "source": [
    "## LangChain with AmazonKnowledgeBase Retriver and SagemakerEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874edcd-7384-4229-b69d-31c18ff7d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "# Define a custom content handler for SageMaker LLM endpoint\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    # Specify content type for input and output\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    # Method to transform user input into the format expected by SageMaker\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})  # Format input as JSON\n",
    "        return input_str.encode(\"utf-8\")  # Encode to bytes\n",
    "\n",
    "    # Method to process the output from SageMaker\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))  # Decode response JSON\n",
    "        return response_json[\"generated_text\"]  # Extract the generated text from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c02366-2fda-45f8-899d-d068230be038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "# Initialize the retriever to fetch relevant documents from the Amazon Knowledge Base\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=kb_id,  # Specify the Knowledge Base ID to retrieve data from\n",
    "    region_name=variables[\"regionName\"],  # Define the AWS region where the Knowledge Base is located\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": number_of_results  # Set the number of relevant documents to retrieve\n",
    "        }\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b423c31-f61c-462e-9302-de4af56ff729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json \n",
    "from botocore.client import Config\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_aws.llms import SagemakerEndpoint\n",
    "\n",
    "# Initialize content handler for processing model inputs/outputs\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "# Define the query to be answered by the model\n",
    "query = \"What are three sub-tasks in question answering over knowledge bases?\"\n",
    "\n",
    "# Create a SageMaker runtime client to interact with the deployed model endpoint\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Initialize the LLM with the SageMaker endpoint\n",
    "llm = SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint.endpoint_name,  # Specify the SageMaker endpoint name\n",
    "        client=sagemaker_runtime,  # Attach the SageMaker runtime client\n",
    "        model_kwargs=generation_configuration,  # Pass the model configuration parameters\n",
    "        content_handler=content_handler,  # Use the custom content handler for formatting\n",
    "    )\n",
    "\n",
    "# Create a Retrieval-Augmented Generation (RAG) pipeline with a retriever and LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  # Use the initialized LLM for answering queries\n",
    "    retriever=retriever,  # Use the retriever to fetch relevant documents\n",
    "    return_source_documents=True  # Enable returning source documents along with the answer\n",
    ")\n",
    "\n",
    "# Execute the query using the RAG pipeline\n",
    "answer = qa(query)\n",
    "\n",
    "# Print the question and the generated answer\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", answer[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32f3c3-32c9-4c82-beab-a513ce362d3a",
   "metadata": {},
   "source": [
    "## RAG using boto3 and SageMaker invoke_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c97aa-8c66-4559-a65c-a4023eae6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock client to interact with the Bedrock Knowledge Base\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Constants for Knowledge Base ID, SageMaker endpoint, and number of results to retrieve\n",
    "KNOWLEDGE_BASE_ID = kb_id\n",
    "ENDPOINT_NAME = llm_endpoint.endpoint_name\n",
    "NUM_RESULTS = number_of_results\n",
    "\n",
    "# Function to retrieve relevant context from the Bedrock Knowledge Base\n",
    "def retrieve_from_bedrock(query):\n",
    "    \"\"\"Retrieve relevant context from Bedrock Knowledge Base\"\"\"\n",
    "    try:\n",
    "        # Retrieve context based on the query using vector search configuration\n",
    "        response = bedrock_agent_runtime.retrieve(\n",
    "            knowledgeBaseId=KNOWLEDGE_BASE_ID,\n",
    "            retrievalQuery={\n",
    "                'text': query  # The query text to search in the knowledge base\n",
    "            },\n",
    "            retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': NUM_RESULTS  # Adjust based on the number of results required\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        # Extract the 'text' from the retrieval results and return as a list\n",
    "        return [result['content']['text'] for result in response['retrievalResults']]\n",
    "    except Exception as e:\n",
    "        # Raise an error if the retrieval process fails\n",
    "        raise RuntimeError(f\"Bedrock retrieval failed: {str(e)}\")\n",
    "\n",
    "# Function to format the prompt for Llama 3 model using retrieved context\n",
    "def format_prompt(query, context):\n",
    "    \"\"\"Format prompt for Llama 3\"\"\"\n",
    "    # Create the system prompt that includes the context and the user's question\n",
    "    system_prompt = f\"\"\"Use the following context to answer the question. If you don't know the answer, say 'I don't know'.\n",
    "        Context:\n",
    "        {\" \".join(context)}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the complete prompt including system and user instructions\n",
    "    return f\"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {system_prompt}\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Question: {query}\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\".strip()\n",
    "\n",
    "# Function to generate a response from the SageMaker endpoint based on the formatted prompt\n",
    "def generate_response(prompt):\n",
    "    \"\"\"Generate response using SageMaker endpoint\"\"\"\n",
    "    # Initialize SageMaker runtime client\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    # Prepare the payload with prompt and generation parameters\n",
    "    payload = {\n",
    "        \"inputs\": prompt,  # The formatted prompt to pass to the model\n",
    "        \"parameters\": generation_configuration  # Additional parameters for the model (e.g., temperature, tokens)\n",
    "    }\n",
    "    try:\n",
    "        # Call the SageMaker endpoint to generate the response\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=ENDPOINT_NAME,  # SageMaker endpoint name\n",
    "            ContentType='application/json',  # Content type for the request\n",
    "            Body=json.dumps(payload)  # Send the payload as JSON\n",
    "        )\n",
    "\n",
    "        # Parse the response body\n",
    "        result = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "        \n",
    "        # Handle different response formats (list or dictionary)\n",
    "        if isinstance(result, list):\n",
    "            # If the result is a list, extract the generated text from the first element\n",
    "            return result[0]['generated_text']\n",
    "        elif 'generated_text' in result:\n",
    "            # If the result is a dictionary with 'generated_text', return the generated text\n",
    "            return result['generated_text']\n",
    "        elif 'generation' in result:\n",
    "            # Alternative format with 'generation' key\n",
    "            return result['generation']\n",
    "        else:\n",
    "            # Raise an error if the response format is unexpected\n",
    "            raise RuntimeError(\"Unexpected response format\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Raise an error if the generation process fails\n",
    "        raise RuntimeError(f\"Generation failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75167f7-fd5d-4429-9023-67493a7cfd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant context from the Bedrock Knowledge Base based on the query\n",
    "context = retrieve_from_bedrock(query)\n",
    "\n",
    "# Format the prompt by combining the user's query and the retrieved context\n",
    "prompt = format_prompt(query, context)\n",
    "\n",
    "# Generate the response using the formatted prompt by calling the SageMaker endpoint\n",
    "response = generate_response(prompt)\n",
    "\n",
    "# Print the user's query\n",
    "print(\"Question:\", {query})\n",
    "\n",
    "# Uncomment below line if you want to debug and see the retrieved context\n",
    "# print(f\"Context: {context}\")\n",
    "\n",
    "# Print the generated answer from the model based on the query and context\n",
    "print(\"Answer:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
