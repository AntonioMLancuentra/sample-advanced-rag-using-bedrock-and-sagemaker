{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdddd7d-2ee2-41b9-8dff-7f71c344bb53",
   "metadata": {},
   "source": [
    "## Sample structured dataset\n",
    "\n",
    "We will add two tables in a Glue database names \"retail.\" Firstly, two sample CSV files will be loaded to a S3 bucket then new external tables will be created using Amazon Athena.\n",
    "\n",
    "Two tables are added in this notebook:\n",
    "* orders\n",
    "* returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5ce5d-482f-4ecf-8efb-62dc6ffbcbed",
   "metadata": {},
   "source": [
    "#### Prerequites\n",
    "* Create an IAM service role for Amazon LakeFormation integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737a36e-5177-41a0-9ad8-d0b48cfa3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 sqlalchemy langchain langchain-community langchain-aws PyAthena openpyxl -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46966e94-1618-4326-8b28-3a381e12a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "import boto3\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "s3_bucket_name = f\"{account_id}-{region}-s3-retail-db-for-agent-example\"\n",
    "db_name = \"retail\"\n",
    "s3_output = f\"s3://{s3_bucket_name}/athena-query-output/\"\n",
    "\n",
    "%who"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb84a5-0cbe-4af8-84eb-fd5986a99158",
   "metadata": {},
   "source": [
    "#### Create a S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23ae6a-874f-4d5c-b341-bff3696af730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Createa a S3 table bucket\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    response = s3.create_bucket(Bucket=s3_bucket_name,\n",
    "                               CreateBucketConfiguration={'LocationConstraint': region})\n",
    "    print(\"S3 bucket created\")\n",
    "except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "    print(\"S3 bucket bucket already exists\")\n",
    "    # Clean up\n",
    "    #s3tables.delete_table_bucket(tableBucketARN=s3table_bucket_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea982fef-2df8-47fe-bcab-89042934cef7",
   "metadata": {},
   "source": [
    "#### Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fa447-ce5b-4997-be80-41ecab682801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper functino to upload CSV files from Excel files.\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "def load_excel_to_df(source_bucket, source_key):\n",
    "    \"\"\"\n",
    "    Convert Excel file from S3 to CSV and save back to S3.\n",
    "    \n",
    "    Args:\n",
    "        source_bucket (str): Source S3 bucket name\n",
    "        source_key (str): Source file key (path to xlsx file)\n",
    "        target_bucket (str): Target S3 bucket name\n",
    "        target_prefix (str): Target prefix (folder) for CSV files\n",
    "    \"\"\"\n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    # Read the Excel file from S3\n",
    "    response = s3_client.get_object(Bucket=source_bucket, Key=source_key)\n",
    "\n",
    "    excel_data = response['Body'].read()\n",
    "\n",
    "    # Load Excel file into pandas\n",
    "    excel_file = pd.ExcelFile(BytesIO(excel_data))\n",
    "\n",
    "    df = pd.read_excel(excel_file, sheet_name=excel_file.sheet_names[0])\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    print(df.head(5))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Function load_excel_to_df has been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f3b19-4e44-4b8f-b5f3-5a1f3d43bb04",
   "metadata": {},
   "source": [
    "#### Create a Glue Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da3744-554a-4fc3-8a40-fca4d2bd26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create Glue client\n",
    "    glue_client = boto3.client('glue')\n",
    "    \n",
    "    # Prepare database input parameters\n",
    "    database_input = {\n",
    "        'Name': db_name\n",
    "    }\n",
    "    \n",
    "    # Create the database\n",
    "    response = glue_client.create_database(\n",
    "        DatabaseInput=database_input\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully created database: {db_name}\")\n",
    "    \n",
    "except glue_client.exceptions.AlreadyExistsException:\n",
    "        print(f\"Database {db_name} already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905121f0-92f9-4c78-92af-78097cbf652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data in to DataFrames\n",
    "order_table_s3_key = \"artifacts/aws-blog-joining-across-quicksight/orders.xlsx\"\n",
    "returns_table_s3_key = \"artifacts/aws-blog-joining-across-quicksight/returns.xlsx\"\n",
    "source_bucket_name = \"aws-bigdata-blog\"\n",
    "\n",
    "\n",
    "df_orders = load_excel_to_df(source_bucket_name, order_table_s3_key)\n",
    "df_returns = load_excel_to_df(source_bucket_name, returns_table_s3_key)\n",
    "\n",
    "df_orders.to_csv(f\"s3://{s3_bucket_name}/data/orders/orders.csv\", index=False)\n",
    "df_returns.to_csv(f\"s3://{s3_bucket_name}/data/returns/returns.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f0c14-b0fe-4472-a4a8-340496a2eb7d",
   "metadata": {},
   "source": [
    "#### Create external tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2255906-1448-4432-a529-16b85e870199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "\n",
    "orders_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS orders (\n",
    "        row_id BIGINT,\n",
    "        order_id STRING,\n",
    "        order_date TIMESTAMP,\n",
    "        ship_date TIMESTAMP,\n",
    "        ship_mode STRING,\n",
    "        customer_id STRING,\n",
    "        customer_name STRING,\n",
    "        segment STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        postal_code DOUBLE,\n",
    "        market STRING,\n",
    "        region STRING,\n",
    "        product_id STRING,\n",
    "        category STRING,\n",
    "        `sub-category` STRING,\n",
    "        product_name STRING,\n",
    "        sales DOUBLE,\n",
    "        quantity BIGINT,\n",
    "        discount DOUBLE,\n",
    "        profit DOUBLE,\n",
    "        shipping_cost DOUBLE,\n",
    "        order_priority STRING\n",
    "    )\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ','\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    LOCATION 's3://{account_id}-{region}-s3-retail-db-for-agent-example/data/orders'\n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\"\n",
    "\n",
    "returns_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS returns (\n",
    "        returned STRING,\n",
    "        order_id STRING,\n",
    "        market STRING\n",
    "    )\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ','\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    LOCATION 's3://{account_id}-{region}-s3-retail-db-for-agent-example/data/returns'\n",
    "    TBLPROPERTIES ('skip.header.line.count'='1')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a50d70-132e-4b09-b1c9-73851bb689f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def execute_athena_query(query, database=db_name, s3_output=s3_output):\n",
    "    # Create an Athena client\n",
    "    athena_client = boto3.client('athena')\n",
    "    \n",
    "    # Start the query execution\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_output\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get the query execution ID\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    \n",
    "    # Wait for the query to complete\n",
    "    while True:\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        \n",
    "        if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "            break\n",
    "            \n",
    "        time.sleep(1)  # Wait for 1 second before checking again\n",
    "    \n",
    "    # If query succeeded, get the results\n",
    "    if state == 'SUCCEEDED':\n",
    "        # Initialize paginator\n",
    "        results = []\n",
    "        paginator = athena_client.get_paginator('get_query_results')\n",
    "        for page in paginator.paginate(QueryExecutionId=query_execution_id):\n",
    "            for row in page['ResultSet']['Rows']:\n",
    "                results.append([field.get('VarCharValue', '') for field in row['Data']])\n",
    "                 \n",
    "        # Get results\n",
    "        if results:\n",
    "            df = pd.DataFrame(results[1:], columns = results[0])\n",
    "            display(df)\n",
    "        else:\n",
    "            print(\"Successfully executed.\")\n",
    "    else:\n",
    "        error_message = response['QueryExecution']['Status'].get('StateChangeReason', 'No error message available')\n",
    "        raise Exception(f\"Query failed: {error_message}\")\n",
    "print(\"Function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f3f05-7851-4027-bcce-0c3a06c5afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_athena_query(orders_sql)\n",
    "execute_athena_query(returns_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e43c29-4387-4c42-bfb0-bded1671e1db",
   "metadata": {},
   "source": [
    "#### Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f201558-4a18-4323-acf2-d67f2b168945",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_athena_query(\n",
    "    \"SELECT * FROM orders LIMIT 5\"\n",
    ")\n",
    "execute_athena_query(\n",
    "    \"SELECT * FROM returns LIMIT 5\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
