{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c01279f-f6b3-46a2-9c76-ecf34f9dc275",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with Hierarchical Chunking Strategy\n",
    "#### What will we do in this workshop?\n",
    "1. Create a Knowledgebase (KB) in the vector database.\n",
    "2. We will create a data source for the KB. The data source will be the Amazon Science and 10K documents stored in S3.\n",
    "3. We will ingest the data from S3, use Hierarchical Chunking to chunk the data, generate vector embeddings, and store the chunks and their corresponding vector embeddings in the KB.\n",
    "4. We will then ask some questions and query the KB to return some chunks and inspect relevancy score.\n",
    "<br>Note: We are not sending the query and its chunks to a LLM in this notebook. We will do that in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0a202-87f1-4a0f-9d2f-e697e6b98bee",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "\n",
    "**Hierarchical Chunking**: Organizes your data into a hierarchical structure, allowing for more granular and efficient retrieval based on the inherent relationships within your data. \n",
    "\n",
    "Organizing your data into a hierarchical structure enables your **RAG (Retrieval-Augmented Generation)** workflow to efficiently navigate and retrieve information from complex, nested datasets. After documents are parsed, the first step is to **chunk** them based on the **parent** and **child chunking size**. \n",
    "\n",
    "- **Parent chunks (higher level)** represent larger segments, such as entire documents or sections.\n",
    "- **Child chunks (lower level)** represent smaller segments, such as paragraphs or sentences.\n",
    "\n",
    "The relationship between parent and child chunks is maintained, allowing for **efficient retrieval and navigation** of the corpus.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Efficient Retrieval**: The hierarchical structure enables faster and more targeted retrieval of relevant information by first performing a **semantic search** on child chunks and then returning the parent chunk. By replacing child chunks with parent chunks, we provide **larger and more comprehensive context** to the foundation model (FM).\n",
    "- **Context Preservation**: Organizing the corpus hierarchically helps maintain contextual relationships between chunks, ensuring more **coherent and contextually relevant** text generation.\n",
    "\n",
    "> **Note:** In hierarchical chunking, **parent chunks** are returned while **search is performed on child chunks**. As a result, you may see **fewer search results**, since one parent can have multiple child chunks.\n",
    "\n",
    "### **Best Use Cases**\n",
    "Hierarchical chunking is best suited for **complex documents** with a nested or hierarchical structure, such as:\n",
    "- **Technical manuals**\n",
    "- **Legal documents**\n",
    "- **Academic papers** with complex formatting and nested tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67e01c-6f6a-4b11-bb8f-256fb15ef2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a module with few helper functions. \n",
    "# These functions will help us create knowledge base (KB), create data source for KB, and ingest data using semantic chunking to KB.\n",
    "\n",
    "import importlib\n",
    "import advanced_rag_utils\n",
    "\n",
    "# Reload module\n",
    "importlib.reload(advanced_rag_utils)\n",
    "\n",
    "# Re-import all functions\n",
    "from advanced_rag_utils import *\n",
    "\n",
    "from datetime import datetime, timedelta, UTC\n",
    "\n",
    "notebook_start_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b874664-bd21-4929-8fe0-165da697e11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's load the variables we saved in the first notebook. We will use these variables\n",
    "import json\n",
    "with open(\"../variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b88f7-04cb-4a2f-80c0-5957e723c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe related to costs from a csv file (if it already exists)\n",
    "df_costs = load_df_from_csv()\n",
    "df_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805c37f-2e62-4945-b0e3-05af7db0e60d",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base\n",
    "Let's specify  chunking strategy, name and description for Knowledge Base (KB) and create a KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ac212-083b-4bd0-abf1-e73919a054f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "kb_chunking_strategy = \"hierarchical\" # [\"fixed\", \"hierarchical\", \"semantic\", \"custom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c4796-f812-49c1-8188-962cadd6eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_name = f\"advanced-rag-workshop-{kb_chunking_strategy}-chunking\"\n",
    "\n",
    "kb_description = \"Knowledge base using Amazon OpenSearch Service as a vector store\"\n",
    "\n",
    "kb = create_kb(kb_name, kb_description, kb_chunking_strategy, variables, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfebc0-bdeb-4f43-a0a0-921d34be6aba",
   "metadata": {},
   "source": [
    "### 2. Create Datasource for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc990e-b0da-4853-a48a-071219d1461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name = f\"advanced-rag-example-{kb_chunking_strategy}\"\n",
    "\n",
    "ds_object = create_data_source_for_kb(kb_chunking_strategy, data_source_name, kb, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d415f-d61f-49ae-9d9a-1ea3063a1452",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cc69d",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierarchical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in a Knowledge Base (KB) in OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e084b1-9c83-45b2-a08f-36b8d2bf0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "ingestion_start_time = datetime.now(UTC)\n",
    "\n",
    "create_ingestion_job(kb, ds_object, variables)\n",
    "\n",
    "ingestion_end_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a5d31-22fa-4a23-be51-b479d02da181",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken = (ingestion_end_time-ingestion_start_time).total_seconds()\n",
    "print(f\"time taken to ingest into KB = {fmt_n(time_taken)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db1388-55e0-4db9-a05d-b7c2f2cf5379",
   "metadata": {},
   "source": [
    "## Embedding LLM Costs\n",
    "1. Specify model id\n",
    "2. Specify start and end time\n",
    "3. Invoke a helper function to query cloud watch\n",
    "5. Calculate costs (please note that pricing is subject to change per region and over time)\n",
    "\n",
    "<br>![Embedding LLM Input Token Costs](./Input_token_embedding_llm_costs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc155e-d7c2-4979-a09d-997af630e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_embedding_cost = get_bedrock_token_based_cost(model_id, ingestion_start_time, ingestion_end_time)\n",
    "print(json.dumps(vector_store_embedding_cost, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cf00a-ae49-4d1c-8ec5-9c847d9947a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add or update the cost into to dataframe. \n",
    "# This will help us compare the costs from various chunking strategies visually.\n",
    "new_row = {\n",
    "    'chunking_algo': kb_chunking_strategy,\n",
    "    'embedding_seconds': vector_store_embedding_cost['duration in minutes']*60,\n",
    "    'input_tokens': vector_store_embedding_cost['input_tokens'],\n",
    "    'invocation_count': vector_store_embedding_cost['invocation_count'],\n",
    "    'total_token_costs': vector_store_embedding_cost['total token costs']\n",
    "}\n",
    "df_costs = update_or_add_row(df_costs, new_row)\n",
    "df_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6709d5-79db-4d82-8117-4668d0d8f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the df\n",
    "save_df_to_csv(df_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988051f4-2741-4180-af71-d445f2eaf37d",
   "metadata": {},
   "source": [
    "### 4. Retrieve: Use input query to RETRIEVE chunks from Vector Database\n",
    "We will use a helper function where you can specify the number of chunks to extract.<br>\n",
    "The helper function will 1/ generate a vector embedding for the query, 2/ search the vector embedding in the Knowledge Base (KB) vector database, 3/ get the number of chunks specified, 4/ Optionally, you can also specify minimum score for similarity in which case the helper function will get chunks with at least the minimum relevancy.\n",
    "\n",
    "<b>Warning: After data is ingested into a KB, when you query immediately, the results might be empty because of eventual consistency. If that happens, please wait for a few seconds and then retry.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f992ba-d37d-4c3f-89d3-128f3a8920ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's pick the chunks with some minimum relevance score for the same question.\n",
    "query = \"Who is the CEO, CFO, and CTO of Amazon?\"\n",
    "\n",
    "#specify the number of chunks\n",
    "n_chunks = 5\n",
    "\n",
    "#Let's specify a minimum similarity score. We should see less chunks retrieved as compared to the previous invocation.\n",
    "min_score = 0.60\n",
    "\n",
    "# get chunks from KB\n",
    "chunks_from_kb = retrieve_from_kb(query, kb, n_chunks, variables, min_score)\n",
    "\n",
    "print(json.dumps(chunks_from_kb, indent=2))\n",
    "\n",
    "# You should see less number of chunks retrieved as compared to the previous cell \n",
    "# because of the minimum relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b2372-8932-4df2-8c73-b5960655752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's summarize with total chunks, minimum score, maximum score, average score, \n",
    "# and lastly the number of chunks with a score more than a specified threshold.\n",
    "score_threshold = 0.40\n",
    "score_structure = analyze_chunk_scores_above_threshold(chunks_from_kb, score_threshold)\n",
    "print(json.dumps(score_structure, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d785feb-6bcd-41c0-bf56-f9b99d325151",
   "metadata": {},
   "source": [
    "### Cost Summary for Running This Notebook\n",
    "In this notebook, we have used an embedding LLM for two purposes. \n",
    "1. Populate a vector store for six PDF files and one CSV file. (7 documents in total)\n",
    "2. Generate a query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434aba2-ccef-4531-a50d-9734777d1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from advanced_rag_utils import embedding_cost_report\n",
    "from time import sleep\n",
    "# Marking notebook endtime\n",
    "\n",
    "sleep(5)\n",
    "notebook_end_time = datetime.now(UTC)\n",
    "\n",
    "cost_for_notebook = get_bedrock_token_based_cost(model_id, notebook_start_time, notebook_end_time)\n",
    "\n",
    "# Your assumptions for your use case:\n",
    "scenario_number_of_documents = 100000\n",
    "scenario_number_of_queries =   5000000\n",
    " \n",
    "display(Markdown(embedding_cost_report(vector_store_embedding_cost, cost_for_notebook, scenario_number_of_documents, scenario_number_of_queries)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
