{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61938163-1eb5-42b8-96d7-7937cb1dcfda",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with SageMaker Endpoint LLM\n",
    "\n",
    "## Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using a SageMaker-hosted large language model (LLM). We will retrieve relevant documents from a knowledge base and use the LLM to generate responses based on the retrieved information.  \n",
    "\n",
    "## Key Steps:  \n",
    "- Configure and query a knowledge base for relevant documents.  \n",
    "- Use a SageMaker-hosted LLM to generate contextual responses.  \n",
    "- Optimize retrieval and generation parameters for improved accuracy.  \n",
    "\n",
    "By the end of this notebook, you'll understand how to integrate SageMaker-hosted models into a RAG pipeline to enhance answer generation with domain-specific knowledge.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d738b9a-e03a-48eb-a296-37dba1bfc52d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import advanced_rag_utils\n",
    "\n",
    "# Reload module\n",
    "importlib.reload(advanced_rag_utils)\n",
    "\n",
    "# Re-import all functions\n",
    "from advanced_rag_utils import *\n",
    "\n",
    "from datetime import datetime, timedelta, UTC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6a65d4-f2e9-4f4c-98b4-1d43e162ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq sagemaker boto3 langchain-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de4aef-bbd1-47f3-b4f3-6a69a238cfe0",
   "metadata": {},
   "source": [
    "Fetching existing resource information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27bb976f-39c7-442a-90a9-8261683f0016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accountNumber': '989679345636',\n",
       " 'regionName': 'us-west-2',\n",
       " 'collectionArn': 'arn:aws:aoss:us-west-2:989679345636:collection/ny2d41n7rmju74rh4ue2',\n",
       " 'collectionId': 'ny2d41n7rmju74rh4ue2',\n",
       " 'vectorIndexName': 'ws-index-',\n",
       " 'bedrockExecutionRoleArn': 'arn:aws:iam::989679345636:role/advanced-rag-workshop-bedrock_execution_role-us-west-2',\n",
       " 's3Bucket': '989679345636-us-west-2-advanced-rag-workshop',\n",
       " 'kbFixedChunk': 'TYG3IXCHCX',\n",
       " 'kbSemanticChunk': 'N7ZHYZVLOX',\n",
       " 'kbHierarchicalChunk': 'UDPUVOULM1',\n",
       " 'kbCustomChunk': 'AD07GOEBQ2'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load variables from JSON file\n",
    "with open(\"../variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86824309-c68a-4845-bcef-a75a91f3e1a5",
   "metadata": {},
   "source": [
    "In this example, you will use a model from [SageMaker Jumpstart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html). Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select FMs quickly based on pre-defined quality and responsibility metrics to perform tasks like article summarization and image generation.\n",
    "\n",
    "To load a model from SageMaker Jumpstart you need to specify a `model_id` and a `model_version`. The current list of models and versions can be found [here](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html).\n",
    "\n",
    "The Llama 3.2 3B model has a `model_id` of `meta-textgeneration-llama-3-2-3b-instruct`. To always use the latest version of the model, you can set `model_version` to `*`, but pining to a specific version is recommended to ensure consistency.\n",
    "\n",
    "Llama 3.2 3B was selected for this example because it is small, fast, and still supports a long context length (128k) to support larger retrievals if necessary for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4588ac-1fb5-4e88-aa00-17bf4fbe7f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LLM Configuration  \n",
    "model_id = \"meta-textgeneration-llama-3-8b-instruct\" \n",
    "model_version = \"2.11.2\"\n",
    "instance_type = \"ml.g5.4xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaef16a",
   "metadata": {},
   "source": [
    "> **Note**: The model deployment process for the SageMaker endpoint will take approximately 8-10 minutes to complete. During this time, the system is:\n",
    "> 1. Provisioning the required compute resources (GPU instances)\n",
    "> 2. Downloading and installing the model artifacts\n",
    "> 3. Configuring the inference environment\n",
    "> 4. Setting up auto-scaling and monitoring for the endpoint\n",
    ">\n",
    "> No further action is needed during this time. The cell will continue to execute until the endpoint is fully deployed and ready for inference. This is a one-time setup that will be used throughout the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b20928-cd3a-4243-9f0c-0fb164cd0d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model 'meta-textgeneration-llama-3-8b-instruct' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llama3Eula.txt for terms of use.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/29/25 22:23:28] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Model <span style=\"color: #008700; text-decoration-color: #008700\">'meta-textgeneration-llama-3-8b-instruct'</span> requires accepting        <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/utils.py#597\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">597</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         end-user license agreement <span style=\"font-weight: bold\">(</span>EULA<span style=\"font-weight: bold\">)</span>. See                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMeta</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">data/eula/llama3Eula.txt</span> for terms of use.                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/29/25 22:23:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Model \u001b[38;2;0;135;0m'meta-textgeneration-llama-3-8b-instruct'\u001b[0m requires accepting        \u001b]8;id=904004;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=484053;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/utils.py#597\u001b\\\u001b[2m597\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         end-user license agreement \u001b[1m(\u001b[0mEULA\u001b[1m)\u001b[0m. See                                    \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMeta\u001b[0m \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mdata/eula/llama3Eula.txt\u001b[0m for terms of use.                                \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'meta-textgeneration-llama-3-8b-instruct' with version '2.11.2'. You can upgrade to version '2.11.3' to get the latest model specifications. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Using model <span style=\"color: #008700; text-decoration-color: #008700\">'meta-textgeneration-llama-3-8b-instruct'</span> with version        <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/utils.py#613\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">613</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008700; text-decoration-color: #008700\">'2.11.2'</span>. You can upgrade to version <span style=\"color: #008700; text-decoration-color: #008700\">'2.11.3'</span> to get the latest model     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         specifications. Note that models may have different input/output          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         signatures after a major version upgrade.                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Using model \u001b[38;2;0;135;0m'meta-textgeneration-llama-3-8b-instruct'\u001b[0m with version        \u001b]8;id=654887;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=769462;file:///opt/conda/lib/python3.11/site-packages/sagemaker/jumpstart/utils.py#613\u001b\\\u001b[2m613\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[38;2;0;135;0m'2.11.2'\u001b[0m. You can upgrade to version \u001b[38;2;0;135;0m'2.11.3'\u001b[0m to get the latest model     \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         specifications. Note that models may have different input/output          \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         signatures after a major version upgrade.                                 \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name:                                              <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         meta-textgeneration-llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8b-instruct-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-04-29-22-23-28-289        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name:                                              \u001b]8;id=1207;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=463335;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         meta-textgeneration-llama-\u001b[1;36m3\u001b[0m-8b-instruct-\u001b[1;36m2025\u001b[0m-04-29-22-23-28-289        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/29/25 22:23:29] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#6019\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6019</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         endpoint-meta-textgeneration-llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8b-instruct-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/29/25 22:23:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=972912;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=157477;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#6019\u001b\\\u001b[2m6019\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         endpoint-meta-textgeneration-llama-\u001b[1;36m3\u001b[0m-8b-instruct-\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m29\u001b[0m-\u001b[1;36m22\u001b[0m-\u001b[1;36m23\u001b[0m-\u001b[1;36m26\u001b[0m   \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (ValidationException) when calling the CreateEndpointConfig operation: 1 validation error detected: Value 'endpoint-meta-textgeneration-llama-3-8b-instruct-2025-04-29-22-23-26' at 'endpointConfigName' failed to satisfy constraint: Member must have length less than or equal to 63\n",
      "New endpoint cannot be created. Looking for any existing endpoints...\n"
     ]
    }
   ],
   "source": [
    "# Deploy or find an existing SageMaker endpoint\n",
    "llm_endpoint_name = get_or_deploy_sagemaker_endpoint(\n",
    "    model_id=model_id, \n",
    "    model_version=model_version, \n",
    "    instance_type=instance_type,\n",
    "    region_name=variables[\"regionName\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c694a-e5e2-42f3-a58b-60eb4b94b5d9",
   "metadata": {},
   "source": [
    "#### Check the progress of a SageMaker Endpoint deployment [here](https://console.aws.amazon.com/sagemaker/home#/endpoints). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83fea6-8500-439e-8945-7e56b885cf21",
   "metadata": {},
   "source": [
    "Store the SageMaker endpoint name for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09242d1-4586-43e3-83ef-1243d4cd7327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the SageMaker endpoint name to the variables JSON file\n",
    "variables = save_sagemaker_endpoint_to_variables(\n",
    "    variables=variables,\n",
    "    endpoint_name=llm_endpoint_name\n",
    ")\n",
    "\n",
    "# Display the endpoint name\n",
    "llm_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd431c1e-4691-40b3-804e-10e09576fe19",
   "metadata": {},
   "source": [
    "# Retrieval and Generation using Bedrock Knowledge Bases and SageMaker hosted models\n",
    "\n",
    "With your endpoint successfully created, you can now use it as an output model in your RAG workflow. The following examples use the Amazon Bedrock Knowledge Bases that you created earlier for retrieval, combined with your SageMaker hosted model for generation. This hybrid approach results in a robust solution, combining the ease of use and managed aspects of Bedrock Knowledge Bases with the model flexibility and configuration controls of SageMaker hosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e138a0-75e5-4b02-ac48-e875a18ce5ca",
   "metadata": {},
   "source": [
    "## RAG Orchestration with LangChain\n",
    "\n",
    "To integrate LangChain with SageMaker endpoints, you first need to define a `ContentHandler`. Its purpose is to perform any transformations of the input/output data to match what the model expects and provide a processed output to client applications.\n",
    "\n",
    "This content handler specifies the input/output content types as UTF-8 encoded `application/json` and pulls the `generated_text` parameter from the json response as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874edcd-7384-4229-b69d-31c18ff7d8ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create content handler for SageMaker endpoint\n",
    "content_handler = create_sagemaker_content_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e6d41-68e5-47f5-9aa2-225e3ac12569",
   "metadata": {},
   "source": [
    "With your `ContentHandler` defined, the next step is to setup your retriever. This retriever is responsible for fetching the results from your Bedrock Knowledge Base so it can be provided as contextual input for generation.\n",
    "\n",
    "The `AmazonKnowledgeBasesRetriever` takes in a parameter of `knowledge_base_id` to select the appropriate knowledge base.  In this example the ids of `kbFixedChunk`, `kbHierarchicalChunk`, `kbSemanticChunk` refer to saved variables in your `variables.json` file that hold the actual knowledge base id.\n",
    "\n",
    "It also takes a `retrieval_config`, which at this time consists of a `vectorSearchConfiguration` with `numberOfResults` as the only configurable parameter. The `numberOfResults` parameter controls the maximum number of search results from the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c02366-2fda-45f8-899d-d068230be038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knowledge Base Selection and configuration\n",
    "kb_id = variables[\"kbSemanticChunk\"]\n",
    "number_of_results = 3\n",
    "\n",
    "# Create retriever for Bedrock Knowledge Base\n",
    "retriever = create_bedrock_retriever(\n",
    "    kb_id=kb_id,\n",
    "    number_of_results=number_of_results,\n",
    "    region_name=variables[\"regionName\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abec14-766d-463f-9b01-cab8db265a7d",
   "metadata": {},
   "source": [
    "Next, define a prompt template for your call to the output model. \n",
    "\n",
    "Since you are using a Llama-3 model in this example, it needs to follow the [correct prompt format](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/).\n",
    "\n",
    "This template uses the following roles:\n",
    "- `system`: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that help the model respond effectively.\n",
    "- `user`: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.\n",
    "- `assistant`: Represents the response generated by the AI model based on the context provided in the system and user prompts.\n",
    "\n",
    "The fields `{context}` and `{question}` in the template will by dynamically injected as part of your RAG chain in a later step. These names are not hardcoded, but need to match what you specify when you build your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfe7e1-a41c-4718-8add-33f52bd81224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get prompt template for Llama model\n",
    "prompt_template = get_llama_prompt_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2284175-f75e-47d7-a528-5e2172e7c2c7",
   "metadata": {},
   "source": [
    "Specify the parameters for generation.\n",
    "\n",
    "`temperature` – Affects the shape of the probability distribution for the predicted output and influences the likelihood of the model selecting lower-probability outputs.\n",
    "    - Choose a lower value to influence the model to select higher-probability outputs.\n",
    "    - Choose a higher value to influence the model to select lower-probability outputs.\n",
    "    - In technical terms, the temperature modulates the probability mass function for the next token. A lower temperature steepens the function and leads to more deterministic responses, and a higher temperature flattens the function and leads to more random responses.\n",
    "\n",
    "`top_k` – The number of most-likely candidates that the model considers for the next token.\n",
    "    - Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.\n",
    "    - Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs.\n",
    "    - For example, if you choose a value of 50 for Top K, the model selects from 50 of the most probable tokens that could be next in the sequence.\n",
    "\n",
    "`top_p` – The percentage of most-likely candidates that the model considers for the next token.\n",
    "    - Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.\n",
    "    - Choose a higher value to increase the size of the pool and allow the model to consider less likely outputs.\n",
    "    - In technical terms, the model computes the cumulative probability distribution for the set of responses and considers only the top P% of the distribution. For example, if you choose a value of 0.8 for Top P, the model selects from the top 80% of the probability distribution of tokens that could be next in the sequence.\n",
    "\n",
    "`max_new_tokens` - The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "\n",
    "`stop` - Specify sequences of characters that stop the model from generating further tokens. If the model generates a stop sequence that you specify, it will stop generating after that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad50832-ce6c-4f26-8ede-11cd864185e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default generation configuration for SageMaker\n",
    "generation_configuration =get_default_sagemaker_generation_config(\n",
    "    temperature=0,\n",
    "    top_k=10,\n",
    "    max_new_tokens=512,\n",
    "    stop=\"<|eot_id|>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2493022-89ef-4e48-bf83-78cc7bce5e01",
   "metadata": {},
   "source": [
    "Here you will create your chain.\n",
    "\n",
    "1. Initialize the `ContentHandler` from above\n",
    "2. Create a `sagemaker-runtime` boto3 client for calling the endpoint\n",
    "3. Initialize the `PromptTemplate` from above\n",
    "4. Define a function to process the documents from the retriever. In this example, the document array is iterated through and the content is joined together using `\\n\\n` between them to break up the context.\n",
    "5. Finally, define your chain. Here, you'll define your chain using LangChain's [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/lcel/) to replace deprecated methods like [RetrievalQA](https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/). LCEL is designed to streamline the process of building useful apps with LLMs and combining related components.\n",
    "\n",
    "Your `qa_chain` will fill pass the `question` parameter from the invocation of the chain, and the context parameter by invoking the retriever and processing the result with the `format_docs` function. From there, those outputs are piped to the prompt template to fill in the defined placeholders, then sent to the `llm` SageMaker endpoint for generation. Finally, the model output is sent to the `StrOutputParser` to convert into a usable string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b423c31-f61c-462e-9302-de4af56ff729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create SageMaker LLM\n",
    "llm = create_sagemaker_llm(\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    generation_config=generation_configuration,\n",
    "    content_handler=content_handler,\n",
    "    region_name=variables[\"regionName\"]\n",
    ")\n",
    "\n",
    "# Create RAG chain\n",
    "qa_chain = aru.create_rag_chain(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    prompt_template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e0412-6080-4dce-909d-a0845ffda874",
   "metadata": {},
   "source": [
    "You can now test your model with an example query. This query will get converted to an embedding and used for Knowledge Base search prior to question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bba00f-fad7-454e-bb23-4121e2fe3da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"In CONSOLIDATED STATEMENTS OF CASH FLOWS, How much did net income change in years 2022, 2023, 2024?\"\n",
    "\n",
    "# Invoke RAG chain\n",
    "response = invoke_rag_chain(qa_chain, query)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32f3c3-32c9-4c82-beab-a513ce362d3a",
   "metadata": {},
   "source": [
    "## RAG using boto3\n",
    "\n",
    "If you are not using LangChain, you can still perform the same tasks using the standard boto3 apis. This example shows how to use the Bedrock Knowledge Base `retrieve` API for retrieval, manually building the generation prompt, then using the SageMaker `invoke_endpoint` API to generate the output. This approach provides the most flexibility by leveraging low level constructs to build your own orchestration flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365830f-b266-4276-8e99-2e5b6566af6d",
   "metadata": {},
   "source": [
    "First, set up resources using configuration from above and define the boto3 client for Bedrock, you'll use this to perform retrievals from your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c97aa-8c66-4559-a65c-a4023eae6567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize constants\n",
    "KNOWLEDGE_BASE_ID = kb_id\n",
    "ENDPOINT_NAME = llm_endpoint_name\n",
    "NUM_RESULTS = number_of_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdba3ad-a71b-4877-8202-278b418786b0",
   "metadata": {},
   "source": [
    "Now, let's call our utility function to perform RAG using direct boto3 approach. This combines all the steps: retrieval, prompt formatting, and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75167f7-fd5d-4429-9023-67493a7cfd0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform RAG using boto3\n",
    "query, response = setup_and_run_rag_with_sagemaker(\n",
    "    query=query,\n",
    "    kb_id=KNOWLEDGE_BASE_ID,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    generation_config=generation_configuration,\n",
    "    number_of_results=NUM_RESULTS,\n",
    "    region_name=variables[\"regionName\"]\n",
    ")\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
