{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fe8a71-6170-4caf-a3b9-e578c1c5f201",
   "metadata": {},
   "source": [
    "# Retrieval and Generation with Bedrock Foundational Models\n",
    "\n",
    "### Overview  \n",
    "This notebook demonstrates how to perform retrieval-augmented generation (RAG) using Amazon Bedrock's foundational models. It covers retrieving relevant documents from a knowledge base and generating responses based on the retrieved context.\n",
    "\n",
    "### Build your own Retrieval Augmented Generation (RAG) system\n",
    "When constructing your own retrieval augmented generation (RAG) system, you can leverage a retriever system and a generator system. The retriever can be an embedding model that identifies the relevant chunks from the vector database based on similarity scores. The generator can be a Large Language Model (LLM) that utilizes the model's capability to answer questions based on the retrieved results (also known as chunks). In the following sections, we will provide additional tips on how to optimize the prompts for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66330105-e1f4-46f3-9b36-9f7560407522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import advanced_rag_utils\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "# Reload module\n",
    "importlib.reload(advanced_rag_utils)\n",
    "\n",
    "# Re-import all functions\n",
    "from advanced_rag_utils import *\n",
    "\n",
    "from datetime import datetime, timedelta, UTC\n",
    "\n",
    "notebook_start_time = datetime.now(UTC)\n",
    "# Load variables from JSON file\n",
    "with open(\"../variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03249f43",
   "metadata": {},
   "source": [
    "## RAG with a simple question\n",
    "\n",
    "##### We will ask the question \"In text-to-sql, what are the stages in data generation process?\" <br/>\n",
    "##### We should expect a response from a PDF shown below that includes the three stages shown in picture below.\n",
    "![Image](./image01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12750c99",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199b3bd-3f66-4a29-9929-83cb9efa5723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Knowledge Base ID - Choose from different chunking strategies (Fixed, Hierarchical, or Semantic)\n",
    "kb_id = variables[\"kbFixedChunk\"] \n",
    "\n",
    "# Get the Bedrock Model ARN\n",
    "model_id = get_model_arn(\n",
    "    base_model_id=\"us.amazon.nova-lite-v1:0\",\n",
    "    #base_model_id=\"us.amazon.nova-pro-v1:0\",\n",
    "    account_number=variables['accountNumber'],\n",
    "    region_name=variables['regionName']\n",
    ")\n",
    "\n",
    "# Number of relevant documents to retrieve for RAG\n",
    "number_of_results = 5\n",
    "\n",
    "# Create default generation configuration\n",
    "generation_config = get_default_generation_config(\n",
    "    max_tokens=4096,\n",
    "    temperature=0,\n",
    "    top_p=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29db82",
   "metadata": {},
   "source": [
    "### Retrieve and Generate with a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00f7db-57bc-4a91-b9dc-6ba3fef14917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the query\n",
    "# recollect in notebook 1.3.1 we executed the same query ? \n",
    "query  = \"Who is the CEO, CFO, and CTO of Amazon? While answering the question, only use the data in context. If for any part of the question, you dont find the information in the context, please say I dont know for that part of the question.\"\n",
    "# Perform retrieval-augmented generation (RAG)\n",
    "response = retrieve_and_generate(\n",
    "    query=query,\n",
    "    kb_id=kb_id,\n",
    "    model_id=model_id,\n",
    "    number_of_results=number_of_results,\n",
    "    generation_config=generation_config,\n",
    "    region_name=variables['regionName']\n",
    ")\n",
    "\n",
    "# Display the results with citations\n",
    "display_rag_results(response, show_citations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7e3a8-5b2f-42de-83dd-e85344ffcdcd",
   "metadata": {},
   "source": [
    "### Comparison between chunking strategies: Fixed vs Semantic\n",
    "\n",
    "##### Now, Let's ask a more nuanced question that needs to extract information from a table in the PDF. Also, let's ask it to do some analysis. <br/>\n",
    "##### We will also compare the response quality when you use fixed size chunking vs Semantic chunking.\n",
    "![image02](image02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e88b9-613a-42cc-ac54-f827902f572f",
   "metadata": {},
   "source": [
    "#### A nuanced query with a Fixed-sized chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb27652-e52e-4dcf-a0e1-6c29f473b52c",
   "metadata": {},
   "source": [
    "##### We will ask a question that should answer how net income changed from 2022 to 2023 to 2024.\n",
    "![image03](image03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd4f32-234b-4719-abd5-f50d91206aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configuration for fixed chunking strategy\n",
    "kb_id_fixed = variables[\"kbFixedChunk\"]\n",
    "\n",
    "# Model ID remains the same\n",
    "model_id = get_model_arn(\n",
    "    base_model_id=\"us.amazon.nova-lite-v1:0\",\n",
    "    account_number=variables['accountNumber'],\n",
    "    region_name=variables['regionName']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba9900-a4ea-4f40-95f0-e8ad7b8b621f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the query for comparing net income changes\n",
    "query = \"In CONSOLIDATED STATEMENTS OF CASH FLOWS, How much did net income change in years 2022, 2023, 2024?\"\n",
    "# Perform RAG with fixed chunking strategy\n",
    "response_fixed = retrieve_and_generate(\n",
    "    query=query,\n",
    "    kb_id=kb_id_fixed,\n",
    "    model_id=model_id,\n",
    "    number_of_results=number_of_results,\n",
    "    generation_config=generation_config,\n",
    "    region_name=variables['regionName']\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_rag_results(response_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51bc3d-39ee-4791-a763-f8285f130e29",
   "metadata": {},
   "source": [
    "#### The response above might not be accurate with what it should be.The accurate response should be:\n",
    "\n",
    "> Year 2022 to Year 2023: \\\\$33,147 increase<br/>\n",
    "Year 2023 to Year 2024: \\\\$28,823 increase \n",
    "\n",
    "#### Now Let's execute the same question while using the KB with Semantic Chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc31f91-f6b0-4b7f-9b20-1732db0c2b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configuration for semantic chunking strategy\n",
    "kb_id_semantic = variables[\"kbSemanticChunk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab14a1f-ea0c-4361-80b7-5d315beaad88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enhance the query to request explanation of the calculation\n",
    "query_with_explanation = \"In CONSOLIDATED STATEMENTS OF CASH FLOWS, How much did net income change in years 2022, 2023, 2024? Show me how you did the math.\"\n",
    "# Perform RAG with semantic chunking strategy\n",
    "response_semantic = retrieve_and_generate(\n",
    "    query=query_with_explanation,\n",
    "    kb_id=kb_id_semantic,\n",
    "    model_id=model_id,\n",
    "    number_of_results=number_of_results,\n",
    "    generation_config=generation_config,\n",
    "    region_name=variables['regionName']\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_rag_results(response_semantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd5820-1e42-463d-8da3-0e59cebf3d27",
   "metadata": {},
   "source": [
    "Compare the above results with the accurate response that should be:\n",
    "> Year 2022 to Year 2023: \\\\$33,147 increase <br/>\n",
    "> Year 2023 to Year 2024: \\\\$28,823 increase\n",
    "\n",
    "As you can see here, Semantic Chunking was able to deliver accurate response as compared to Fixed Size chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c92234",
   "metadata": {},
   "source": [
    "## Improve RAG quality with Enhanced Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab045f5",
   "metadata": {},
   "source": [
    "### Importance of Prompt Engineering\n",
    "Prompt engineering refers to the practice of optimizing textual input to a large language model (LLM) to improve output and receive the responses you want. Prompting helps an LLM perform a wide variety of tasks, including classification, question answering, code generation, creative writing, and more. The quality of prompts that you provide to a LLM can impact the quality of the model's responses. <br/>\n",
    " \n",
    "\n",
    "### Useful techniques to improve prompts for Amazon Nova models\n",
    "Please refer [link](https://docs.aws.amazon.com/nova/latest/userguide/prompting.html) for the best practice of prompt engineering with Amazon Nova models. Following are a few highlights:\n",
    "* Create precise prompts. Provide contextual information, speficy the output format and style, and provide clear prompt sections.\n",
    "* Use system propmts to define how the model will repond.\n",
    "* Give Amazon Nova time to think. For example, add ```\"Think step-by-step.\"``` at the end of your query.\n",
    "* Provide examples.\n",
    "\n",
    "### Tips for using prompts in RAG\n",
    "* Provide Prompt Template: As with other functionalities, enhancing the system prompt can be beneficial. You can define the RAG Systems description in the system prompt, outlining the desired persona and behavior for the model.\n",
    "* Use Model Instructions: Additionally, you can include a dedicated ```\"Model Instructions:\"``` section within the system prompt, where you can provide specific guidelines for the model to follow. For instance, you can list instructions such as: ```In this example session, the model has access to search results and a user's question, its job is to answer the user's question using only information from the search results.```\n",
    "* Avoid Hallucination by restricting the instructions: Bring more focus to instructions by clearly mentioning \"DO NOT USE INFORMATION THAT IS NOT IN SEARCH RESULTS!\" as a model instruction so the answers are grounded in the provided context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa9d55-474d-403d-ac4f-ecf6a5914886",
   "metadata": {},
   "source": [
    "#### Without a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676924ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the query about Amazon's financial results\n",
    "query = \"Show me the amazon financial results for 2023\"\n",
    "\n",
    "# Perform RAG without prompt template\n",
    "response_no_template = retrieve_and_generate(\n",
    "    query=query,\n",
    "    kb_id=kb_id,\n",
    "    model_id=model_id,\n",
    "    number_of_results=number_of_results,\n",
    "    generation_config=generation_config,\n",
    "    region_name=variables['regionName']\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_rag_results(response_no_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d97251-3560-48c0-ad95-13f9ea4ca40a",
   "metadata": {},
   "source": [
    "#### Using a Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c4a54-54bc-4aa4-aba2-695d29da0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template for financial analysis\n",
    "prompt_template = \"\"\"\n",
    "You are a professional financial analyst. \n",
    "Based on the retrieved content from Amazon's 10-K filings, provide clear, concise, and insightful answers to user questions. \n",
    "When summarizing financial results, respond in bullet points highlighting key metrics, trends, and takeaways. \n",
    "Ensure your answers are accurate, data-driven, and easy to understand.\n",
    "Format the output as Markdown document.\n",
    "\n",
    "$Query$\n",
    "Resource: $search_results$\n",
    "\"\"\"\n",
    "\n",
    "# Perform RAG with the prompt template\n",
    "response_with_template = retrieve_and_generate(\n",
    "    query=query,\n",
    "    kb_id=kb_id,\n",
    "    model_id=model_id,\n",
    "    number_of_results=number_of_results,\n",
    "    generation_config=generation_config,\n",
    "    prompt_template=prompt_template,\n",
    "    region_name=variables['regionName']\n",
    ")\n",
    "\n",
    "# Display the results as Markdown\n",
    "# display_rag_results(response_with_template, format_as_markdown=True)\n",
    "print('----------------- Answer ---------------------')\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response_with_template['output']['text'].replace(\"$\", \"USD \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f0bce-e2b8-41cc-8d72-fdba7a3ffa14",
   "metadata": {},
   "source": [
    "#### Change the prompt to produce JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02be6e6-b01b-4288-ac8f-93516a60fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the prompt template to request JSON output\n",
    "json_prompt_template = \"\"\"\n",
    "You are a professional financial analyst. \n",
    "Based on the retrieved content from Amazon's 10-K filings, provide clear, concise, and insightful answers to user questions. \n",
    "When summarizing financial results, respond in bullet points highlighting key metrics, trends, and takeaways. \n",
    "Ensure your answers are accurate, data-driven, and easy to understand.\n",
    "Format the output as JSON document.\n",
    "\n",
    "$Query$\n",
    "Resource: $search_results$\n",
    "\"\"\"\n",
    "\n",
    "# Perform RAG with JSON prompt template\n",
    "response = retrieve_and_generate(\n",
    "    query=query,\n",
    "    kb_id=kb_id,\n",
    "    model_id=model_id,\n",
    "    number_of_results=number_of_results,\n",
    "    generation_config=generation_config,\n",
    "    prompt_template=json_prompt_template,\n",
    "    region_name=variables['regionName']\n",
    ")\n",
    "\n",
    "# Display the results as Markdown to properly format the JSON\n",
    "print('----------------- Answer ---------------------')\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response['output']['text'].replace(\"$\", \"\\\\$\")))\n",
    "#display_rag_results(response_json, format_as_markdown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f41ae-f327-406d-aff0-3c7abf56f0ef",
   "metadata": {},
   "source": [
    "### Cost Summary for Running This Notebook\n",
    "In this notebook, we used two LLMs: 1) Embedding 2) Text Generation.\n",
    "\n",
    "Let us breakdown the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99133bd6-2d37-48a5-8222-854a2ab031a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "inference_model_id = \"us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Mark end of query executions here:\n",
    "notebook_end_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac981b-31a9-455e-a959-15e4741e3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notebook_start_time, notebook_end_time)\n",
    "\n",
    "embedding_cost = get_bedrock_token_based_cost(embedding_model_id, notebook_start_time, notebook_end_time)\n",
    "inference_cost = get_bedrock_token_based_cost(inference_model_id, notebook_start_time, notebook_end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46dbd7e-e9b6-4053-9f15-1d33fef15fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5dbac-4de5-4fe6-af3f-6a460a0125de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
