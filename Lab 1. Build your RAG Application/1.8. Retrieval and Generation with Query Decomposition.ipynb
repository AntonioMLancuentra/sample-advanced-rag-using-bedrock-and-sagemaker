{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92c5158-0cce-4ffa-bdab-6a397caba9ed",
   "metadata": {},
   "source": [
    "## Query Decomposition: Avoid semantic delutions in the queries\n",
    " * \"Semantic dilutions\" refer to the loss of meaning or relevance when chunks of text, intended to represent information for retrieval, become too large or lack semantic cohesion, leading to less accurate and useful results.\n",
    " * Semantic dilutions can be mitigated on the indexed documents side by applying optimized chunking strategies per each use case.\n",
    " * Query texts can also have semantic delutions when multiple contexts or topics are combined. This can be improved by reforming the queries.\n",
    "\n",
    "This notebook will demonstrate \"query decomposition\" strategy to improve the search quality against a knowledge base. \n",
    "\n",
    "![Query Decomposition](https://raw.githubusercontent.com/aws-samples/langgraph-agents-with-amazon-bedrock/refs/heads/main/assets/lab3_2.png \"https://github.com/aws-samples/langgraph-agents-with-amazon-bedrock/tree/main/Lab_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd964ae-e143-4ce6-b242-978c4d29bcdc",
   "metadata": {},
   "source": [
    "#### Prerequsites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1782f4-a89d-4044-bff6-05e038aeb7de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install -U boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f773f-683c-4bc4-918b-15a086080bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229c2a7-12f9-4965-aeec-8f869fd41cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configurations for Knowledge Base retrievals\n",
    "\n",
    "kb_id = variables[\"kbSemanticChunk\"]\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-lite-v1:0\"\n",
    "\n",
    "number_of_results = 5\n",
    "generation_configuration = {\n",
    "    'inferenceConfig': {\n",
    "                    'textInferenceConfig': {\n",
    "                        'maxTokens': 1024,\n",
    "                        'stopSequences': [],\n",
    "                        'temperature': 0.0,\n",
    "                        'topP': 0.2\n",
    "                    }\n",
    "                },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebee40",
   "metadata": {},
   "source": [
    "### How Model Size Affects Table Interpretation  \n",
    "\n",
    "When querying Amazon’s Operating Income for 2022, **smaller models (Nova Lite, Llama 3B)** tend to pick the **\"At Prior Year Rates\"** value (\\$11,387), while **larger models (Nova Pro)** correctly select the **\"As Reported\"** value (\\$12,248).\n",
    "\n",
    "#### Possible Reasons:\n",
    "- **Table Parsing Limitations:** Smaller models may not accurately align column headers to values.  \n",
    "- **Context Misinterpretation:** They might default to the last numerical column or fail to strongly associate **\"As Reported\"** with the correct column.  \n",
    "- **Stronger Reasoning in Larger Models:** Nova Pro better understands structured data, leading to more accurate retrieval.  \n",
    "\n",
    "![Image](./operating_income.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5130cdb-5d9f-4c5c-9a68-2352026616fb",
   "metadata": {},
   "source": [
    "### Basic RAG query\n",
    "The query retrieves only one of the topics mentioned. It is because of the mix of two queries dilluted the second topic, which is called \"semantic dillution.\"\n",
    "In real world scenarios, it is required to pull multiple chunks with different contexts, for example:\n",
    "* How Amazon's net income increased from 2018 to 2024?\n",
    "* What is difference between RAG and text-to-SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60064755-7215-4b10-9246-30b53697e677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WITHOUT QUERY DECOMPOSITION\n",
    "import boto3\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Query example\n",
    "query= \"What was Amazon’s Operating Income as reported for the fiscal year ending December 31, 2022? What is text-to-SQL? How did text-to-SQL contribute to Amazons earnings, if any?\"\n",
    "\n",
    "\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": model_id,\n",
    "            \"generationConfiguration\"\n",
    "            : generation_configuration,\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'],end='\\n'*2)\n",
    "print('----------------- Citations ------------------')\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c25c30-3314-4790-a9bc-dac071ebde6b",
   "metadata": {},
   "source": [
    "### Use Amazon Bedrock API feature to decompose a query.\n",
    "ReteriveAndGenerate API support built-in query decompose feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b21ca6-e1e7-4878-b9ff-0c5727f101cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WITH QUERY DECOMPOSITION\n",
    "import boto3\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "model_id = f\"arn:aws:bedrock:us-west-2:{variables['accountNumber']}:inference-profile/us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "# Query example\n",
    "query= \"What was Amazon’s Operating Income as reported for the fiscal year ending December 31, 2022? What is text-to-SQL? How did text-to-SQL contribute to Amazons earnings, if any?\"\n",
    "\n",
    "response = bedrock_agent_runtime.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": model_id,\n",
    "            \"generationConfiguration\": generation_configuration,\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": number_of_results\n",
    "                } \n",
    "            },\n",
    "            #######################\n",
    "            'orchestrationConfiguration': {\n",
    "                'queryTransformationConfiguration': {\n",
    "                    'type': 'QUERY_DECOMPOSITION'\n",
    "                }\n",
    "            }\n",
    "            #######################\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print('----------------- Answer ---------------------')\n",
    "print(response['output']['text'],end='\\n'*2)\n",
    "print('----------------- Citations ------------------')\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807e57b-016b-48cd-a53d-cd0314b13d82",
   "metadata": {},
   "source": [
    "## Query Decomposition with Agentic RAG using SageMaker and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbf82a-c1c2-4637-941d-3909211043bb",
   "metadata": {},
   "source": [
    "#### Prerequites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65baa6fd-8797-4265-879e-65b9523789a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "# Reuse the same LLM endpoint deployed to SageMaker in the previous notebook.\n",
    "from langchain_aws.llms import SagemakerEndpoint\n",
    "from langchain_aws.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"]\n",
    "\n",
    "sagemaker_runtime = boto3.client(\n",
    "    \"sagemaker-runtime\"\n",
    ")\n",
    "\n",
    "generation_configuration = {\"temperature\": 0,\n",
    "                            \"top_p\": 0.3,\n",
    "                            \"max_new_tokens\": 512,\n",
    "                            \"stop\":[\"<|eot_id|>\"]\n",
    "                            }\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "        endpoint_name=variables[\"sagemakerLLMEndpoint\"],\n",
    "        client=sagemaker_runtime,\n",
    "        model_kwargs=generation_configuration,\n",
    "        content_handler=ContentHandler(),\n",
    "    )\n",
    "\n",
    "\n",
    "# RAG config\n",
    "number_of_results = 5\n",
    "\n",
    "\n",
    "from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=kb_id,\n",
    "    region_name=variables[\"regionName\"],\n",
    "    retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": number_of_results}},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca334e-6d0e-4763-8c1d-ef5e5199be6c",
   "metadata": {},
   "source": [
    "#### Test a complex query with plain Q&A chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22e3a7-4d3a-4fb4-9350-2d66474b4736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Human:\n",
    "\n",
    "You are an assistant who answers questions using  following pieces of retrieved context only. \n",
    "If you don't find the answer from the retrieved context, do not include it and just say you don't know about it.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer: Based on the context given, my answer for your question is as following:\n",
    "\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be0f15-b378-482c-844c-0e5e6fdb2e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query\n",
    "query= \"What was the Operating Income of Amazon As Reported for the Year Ending December 31, 2022? What is text-to-SQL? How did text-to-SQL contribute to Amazons earnings, if any?\"\n",
    "\n",
    "# Invoke RAG chain\n",
    "answer = qa_chain.invoke(query)\n",
    "\n",
    "#print(\"Question:\", query)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceddc3b-4ca6-4309-ba9a-6a2e5fc4a064",
   "metadata": {},
   "source": [
    "### Query Decomposition using Agentic RAG with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35323e79-5336-4e1d-a5b4-fc274a977ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "\n",
    "# The [bool] desribes a parameterization of a generic.\n",
    "# It's basically indicating what the return type of parse is\n",
    "# in this case the return type is either True or False\n",
    "class CustomOutputParser(BaseOutputParser):\n",
    "    \"\"\"Custom parser.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        print(text)\n",
    "        return text\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"custome_output_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf9355-e0db-4f00-b8c3-4522ba7f55d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.agents import AgentType, initialize_agent, Tool\n",
    "\n",
    "\n",
    "# 1. Define the RAG tools\n",
    "def fn_search(question):\n",
    "    \"\"\"\n",
    "    Search the answer of the question from the knowledge base. \n",
    "    \"\"\"\n",
    "    chunks = [doc.page_content for doc in retriever.invoke(question)]\n",
    "    return chunks\n",
    "\n",
    "def noop(input)-> None: \n",
    "    \"\"\"Use this when no action need to be taken for your thought.\"\"\"\n",
    "    return\n",
    "\n",
    "kb_tool_finance=Tool(\n",
    "    name=\"SearchFinancialStatements\",\n",
    "    func=fn_search,\n",
    "    description=\"Use this tool to find answers for financial data.\"\n",
    ")\n",
    "\n",
    "kb_tool_technology=Tool(\n",
    "    name=\"SearchTechnologyDocuments\",\n",
    "    func=fn_search,\n",
    "    description=\"Use this tool to find answers for technologies.\"\n",
    ")\n",
    "\n",
    "noop=Tool(\n",
    "    name=\"None\",\n",
    "    func=noop,\n",
    "    description=\"Use this when no action need to be taken for your thought\"\n",
    ")\n",
    "\n",
    "tools = [kb_tool_finance, kb_tool_technology, noop]\n",
    "\n",
    "\n",
    "# 2. Create the agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.SELF_ASK_WITH_SEARCH,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "From https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html\n",
    "SELF_ASK_WITH_SEARCH = 'self-ask-with-search'\n",
    "An agent that breaks down a complex question into a series of simpler questions.\n",
    "\n",
    "This agent uses a search tool to look up answers to the simpler questions in order to answer the original complex question.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Test the agent\n",
    "query= \"What was Amazon’s Operating Income as reported for the fiscal year ending December 31, 2022? What is text-to-SQL? How did text-to-SQL contribute to Amazons earnings, if any?\"\n",
    "result = agent.run(query)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
