{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdb93e3-4eea-486b-b5d4-6d4a330a2338",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base with Fixed Chunking Strategy\n",
    "#### What will we do in this workshop?\n",
    "1. Create a Knowledgebase (KB) in the vector database.\n",
    "2. We will create a data source for the KB. The data source will be the Amazon Science and 10K documents stored in S3.\n",
    "3. We will ingest the data from S3, use Fixed Chunking to chunk the data, generate vector embeddings, and store the chunks and their corresponding vector embeddings in the KB.\n",
    "4. We will then ask some questions and query the KB to return some chunks and inspect relevancy score.\n",
    "<br>Note: We are not sending the query and its chunks to a LLM in this notebook. We will do that in other notebooks.\n",
    "![We are generating vector embeddings and storing them in a KB in Vector Database](./Fixed_Chunking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494778cc-5990-465f-aa97-86a4b3ad9620",
   "metadata": {},
   "source": [
    "Chunking data is essential. If you are adding large documents with hundreds of pages to your knowledge base then you need to split them up and return only the relevant sections to use as context for your inference. If you are returning too much context it will increase costs (models charge based on input token count) and latency. It may also harm output quality. Shorter chunks will provide a better match but may lack the context necessary to answer a question.\n",
    "\n",
    "Bedrock Knowledge bases have a few different chunking strategies to choose from. They handle everything from splitting at semantic boundaries like paragraphs and hierarchical structures. However some document types can benefit from custom chunking. For example, any form of mark up can be used by a custom chunking approach.\n",
    "\n",
    "You can also create your own custom chunking approach using a Lambda function. If you want to add any custom metadata then you will need to add a Lambda function. You can either handle the chunking yourself, edit an existing chunk or just add metadata. Metadata can then be used for filtering.\n",
    "\n",
    "It is important to tune your chunking to the type of documents being ingested. Getting the wrong chunk size will affect the accuracy and response times. It will also increase the costs in both the vector storage and inference steps. The defaults supplied in Bedrock are pretty good but they may need tailored to your specific circumstances. Longer and more technical documents may need larger chunk sizes to make sure they include more context. Speech (like a chat transcript) can benefit from shorter chunks.\n",
    "\n",
    "![Chunking Strategies](./chunking-strategies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd926b-0c3f-43b0-9e32-459389fcb7a3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we will implement a knowledge base using a fixed chunking strategy. Here are the key steps we'll perform:\n",
    "\n",
    "1. **Create a Knowledge Base**: Set up an Amazon Bedrock Knowledge Base with fixed-size chunking configuration that will store and retrieve our vector embeddings.\n",
    "\n",
    "2. **Create a Data Source**: Connect our Knowledge Base to the documents we uploaded to S3 in the previous notebook.\n",
    "\n",
    "3. **Start Ingestion Job**: Begin the process of transforming our documents into chunks, creating embeddings, and storing them in our vector database.\n",
    "\n",
    "4. **Retrieve and Generate**: Test our Knowledge Base by retrieving relevant information based on a sample query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c1927-f3d4-4cb0-b2db-d2fc4fba8aac",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "\n",
    "**Fixed Chunking**: Involves dividing your documents into fixed-size chunks, regardless of the content within them. Each chunk contains a predefined number of tokens or characters, and this method allows for more uniform data organization. \n",
    "\n",
    "![How Fixed Sized Chunking Works](./Fixed_how_it_works.png)\n",
    "\n",
    "Fixed chunking is useful when you want to ensure that your chunks are of a consistent size, making them easier to process and retrieve in a predictable manner. The document is split into sections of equal length, and each section becomes a separate chunk. This method works well when the content is relatively homogeneous, and the chunk boundaries are not as crucial to understanding the underlying context.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Uniformity**: Each chunk has the same size, making the system more predictable. This helps with processing efficiency since you know that each chunk is of a consistent size, making batch operations and parallel processing easier.\n",
    "- **Simplified Retrieval**: Since the chunk sizes are uniform, searching through the data becomes straightforward. You can quickly determine the length of chunks, which can be useful for performance optimization and scalability in large datasets.\n",
    "- **Performance Optimization**: Fixed chunks are ideal when you want to control the computational cost of document retrieval and chunking. Having equal-sized chunks reduces the chance of computational bottlenecks in scenarios requiring large-scale document processing.\n",
    "\n",
    "> **Note:** While fixed chunking can be efficient for certain use cases, it may not preserve the natural semantic boundaries of the content, such as paragraphs or sections. This may lead to chunks that start or end at arbitrary places, potentially cutting off context in the middle of a sentence or idea.\n",
    "\n",
    "### **Best Use Cases**\n",
    "Fixed chunking is suitable for cases where:\n",
    "- **Homogeneous content**: The content is consistent, and boundaries are not as important.\n",
    "- **Performance**: You need uniform-sized chunks for predictable processing or optimization of large-scale systems.\n",
    "- **Simplified text processing**: When chunk boundaries do not need to match natural semantic structures like paragraphs or sentences.\n",
    "\n",
    "Examples include:\n",
    "- **General document indexing**: When large datasets are involved, and uniform chunk sizes optimize retrieval.\n",
    "- **Text summarization**: Fixed chunking is helpful when generating summaries from uniformly sized data pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67e01c-6f6a-4b11-bb8f-256fb15ef2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a module with few helper functions. \n",
    "# These functions will help us create knowledge base (KB), create data source for KB, and ingest data using semantic chunking to KB.\n",
    "\n",
    "import importlib\n",
    "import advanced_rag_utils\n",
    "\n",
    "# Reload module\n",
    "importlib.reload(advanced_rag_utils)\n",
    "\n",
    "# Re-import all functions\n",
    "from advanced_rag_utils import *\n",
    "\n",
    "from datetime import datetime, timedelta, UTC\n",
    "\n",
    "notebook_start_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b874664-bd21-4929-8fe0-165da697e11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's load the variables we saved in the first notebook. We will use these variables\n",
    "import json\n",
    "with open(\"../variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b05117-98b9-4f6a-94e8-51924a3a25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe related to costs from a csv file (if it already exists)\n",
    "df_costs = load_df_from_csv()\n",
    "df_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805c37f-2e62-4945-b0e3-05af7db0e60d",
   "metadata": {},
   "source": [
    "### 1. Create a Knowledge Base\n",
    "Let's specify  chunking strategy, name and descripotion for Knowledge Base (KB) and create a KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ac212-083b-4bd0-abf1-e73919a054f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "kb_chunking_strategy = \"fixed\" # [\"fixed\", \"hierarchical\", \"semantic\", \"custom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c4796-f812-49c1-8188-962cadd6eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_name = f\"advanced-rag-workshop-{kb_chunking_strategy}-chunking\"\n",
    "\n",
    "kb_description = \"Knowledge base using Amazon OpenSearch Service as a vector store\"\n",
    "\n",
    "kb = create_kb(kb_name, kb_description, kb_chunking_strategy, variables, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfebc0-bdeb-4f43-a0a0-921d34be6aba",
   "metadata": {},
   "source": [
    "### 2. Create Datasource for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc990e-b0da-4853-a48a-071219d1461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_name = f\"advanced-rag-example-{kb_chunking_strategy}\"\n",
    "\n",
    "ds_object = create_data_source_for_kb(kb_chunking_strategy, data_source_name, kb, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d415f-d61f-49ae-9d9a-1ea3063a1452",
   "metadata": {},
   "source": [
    "### 3. Start Ingestion Job for Amazon Bedrock Knowledge base pointing to Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cc69d",
   "metadata": {},
   "source": [
    "> **Note**: The ingestion process will take approximately 2-3 minutes to complete. During this time, the system is processing your documents by:\n",
    "> 1. Extracting text from the source files\n",
    "> 2. Chunking the content according to the defined strategy (Fixed / Semantic / Hierachical / Custom)\n",
    "> 3. Generating embeddings for each chunk\n",
    "> 4. Storing the embeddings and associated metadata in a Knowledge Base (KB) in OpenSearch vector database\n",
    ">\n",
    "> You'll see status updates as the process progresses. Please wait for the \"Ingestion job completed successfully\" message before proceeding to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e084b1-9c83-45b2-a08f-36b8d2bf0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "ingestion_start_time = datetime.now(UTC)\n",
    "create_ingestion_job(kb, ds_object, variables)\n",
    "ingestion_end_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e8c41-caed-451b-9be2-8e28f6c7a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken = (ingestion_end_time-ingestion_start_time).total_seconds()\n",
    "print(f\"time taken to ingest into KB = {fmt_n(time_taken)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabeab5-a96d-484f-b172-df1691a6dc52",
   "metadata": {},
   "source": [
    "## Embedding LLM Costs\n",
    "1. Specify model id\n",
    "2. Specify start and end time\n",
    "3. Invoke a helper function to query cloud watch\n",
    "5. Calculate costs (please note that pricing is subject to change per region and over time)\n",
    "\n",
    "<br>![Embedding LLM Input Token Costs](./Input_token_embedding_llm_costs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9d526-9256-4d42-9d89-9c1230ff0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_embedding_cost = get_bedrock_token_based_cost(model_id, ingestion_start_time, ingestion_end_time)\n",
    "print(json.dumps(vector_store_embedding_cost, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61e328-1350-4e44-83d7-352f8cd8d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add or update the cost binfo to dataframe. \n",
    "# This will help us compare the costs from various chunking strategies visually.\n",
    "new_row = {\n",
    "    'chunking_algo': kb_chunking_strategy,\n",
    "    'embedding_seconds': vector_store_embedding_cost['duration in minutes']*60,\n",
    "    'input_tokens': vector_store_embedding_cost['input_tokens'],\n",
    "    'invocation_count': vector_store_embedding_cost['invocation_count'],\n",
    "    'total_token_costs': vector_store_embedding_cost['total token costs']\n",
    "}\n",
    "df_costs = update_or_add_row(df_costs, new_row)\n",
    "df_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1bcab-f6d2-496f-9abc-6e961700ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the df\n",
    "save_df_to_csv(df_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988051f4-2741-4180-af71-d445f2eaf37d",
   "metadata": {},
   "source": [
    "### 4. Retrieve: Use input query to RETRIEVE chunks from Vector Database\n",
    "We will use a helper function where you can specify the number of chunks to extract.<br>\n",
    "The helper function will 1/ generate a vector embedding for the query, 2/ search the vector embedding in the Knowledge Base (KB) vector database, 3/ get the number of chunks specified, 4/ Optionally, you can also specify minimum score for similarity in which case the helper function will get chunks with at least the minimum relevancy.\n",
    "\n",
    "<b>Warning: After data is ingested into a KB, when you query immediately, the results might be empty because of eventual consistency. If that happens, please wait for a few seconds and then retry.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855a3ee-1138-4bff-994c-0b1bf0f60873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ask some completely irrelevant question and see what we get from the KnolwedgeBase (KB) in vector database.\n",
    "query = \"What were the Taco sales in our franchise location on Guadalupe street?\"\n",
    "\n",
    "# specify the number of chunks we want to get from the KB in vector database.\n",
    "n_chunks = 3 \n",
    "\n",
    "# get chunks from KB\n",
    "chunks_from_kb = retrieve_from_kb(query, kb, n_chunks, variables)\n",
    "\n",
    "# print the chunks, metadata, and the score\n",
    "print(json.dumps(chunks_from_kb, indent=4))\n",
    "\n",
    "# You should see a very low score (typically less than 0.4) as there is nothing related to the question in the KB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d823e6b-f4d7-4320-8eee-fbcde680c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's summarize with total chunks, minimum score, maximum score, average score, \n",
    "# and lastly the number of chunks with a score more than a specified threshold.\n",
    "score_threshold = 0.40\n",
    "\n",
    "# We will use the helper function below to iterate through each element in json structure and print the \n",
    "# score statistics for the returned chunks from the KB\n",
    "score_structure = analyze_chunk_scores_above_threshold(chunks_from_kb, score_threshold)\n",
    "\n",
    "print(json.dumps(score_structure, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cf039-2dfe-41e9-9324-ffe789d1298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ask some something more related about Amazon's net incomes.\n",
    "query = \"What were net incomes of Amazon in 2022, 2023 and 2024?\"\n",
    "\n",
    "# specify the number of chunks\n",
    "n_chunks = 5\n",
    "\n",
    "# get chunks from KB\n",
    "chunks_from_kb = retrieve_from_kb(query, kb, n_chunks, variables)\n",
    "\n",
    "#print the chunks, metadata, and the score\n",
    "print(json.dumps(chunks_from_kb, indent=4))\n",
    "\n",
    "# You should see relevant content with relatively higher score as compared to when youa sked an irrelevant question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e15454-286d-4a07-baa2-50f8c13fe8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's summarize with total chunks, minimum score, maximum score, average score, \n",
    "# and lastly the number of chunks with a score more than a specified threshold.\n",
    "score_threshold = 0.56\n",
    "score_structure = analyze_chunk_scores_above_threshold(chunks_from_kb, score_threshold)\n",
    "print(json.dumps(score_structure, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01940994-078e-4396-bd73-2425c93f2b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For the following query. We know that the document does not mention the abbreviations CEO, CFO or CTO but mentions\n",
    "# Chief Executive Officer and Chief Fincnace Officer. Let's see how good is the vector search.\n",
    "query = \"Who is the CEO, CFO, and CTO of Amazon? While answering the question, only use the data in context. If for any part of the question, you dont find the information in the context, please say I dont know for that part of the question.\"\n",
    "\n",
    "#specify the number of chunks\n",
    "n_chunks = 5 \n",
    "\n",
    "# get chunks from KB\n",
    "chunks_from_kb = retrieve_from_kb(query, kb, n_chunks, variables)\n",
    "\n",
    "#print the chunks, metadata, and the score\n",
    "print(json.dumps(chunks_from_kb, indent=2))\n",
    "\n",
    "#You will see that vector search extracts the chunks that contains the word Chief Executive Officer and Chief Financial Officer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc3062-49d1-4f9d-9eee-ceeaa64577c7",
   "metadata": {},
   "source": [
    "#### Note: In the above results, the metadata has the name of the file, page number and other info. Optionally, this is something you could choose to share in your Generative application. Users can then click on the link and learn more from that content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f992ba-d37d-4c3f-89d3-128f3a8920ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's pick the chunks with some minimum relevance score for the same question.\n",
    "query = \"Who is the CEO, CFO, and CTO of Amazon? While answering the question, only use the data in context. If for any part of the question, you dont find the information in the context, please say I dont know for that part of the question.\"\n",
    "\n",
    "#specify the number of chunks\n",
    "n_chunks = 5\n",
    "\n",
    "#Let's specify a minimum similarity score. We should see less chunks retrieved as compared to the previous invocation.\n",
    "min_score = 0.50\n",
    "\n",
    "# get chunks from KB\n",
    "chunks_from_kb = retrieve_from_kb(query, kb, n_chunks, variables, min_score)\n",
    "\n",
    "print(json.dumps(chunks_from_kb, indent=2))\n",
    "\n",
    "# You should see less number of chunks retrieved as compared to the previous cell \n",
    "# because of the minimum relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b2372-8932-4df2-8c73-b5960655752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's summarize with total chunks, minimum score, maximum score, average score, \n",
    "# and lastly the number of chunks with a score more than a specified threshold.\n",
    "score_threshold = 0.55\n",
    "score_structure = analyze_chunk_scores_above_threshold(chunks_from_kb, score_threshold)\n",
    "print(json.dumps(score_structure, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743eb5af-a252-4063-bad8-f6e89c0460b5",
   "metadata": {},
   "source": [
    "### Cost Summary for Running This Notebook\n",
    "In this notebook, we have used an embedding LLM for two purposes. \n",
    "1. Populate a vector store for six PDF files and one CSV file. (7 documents in total)\n",
    "2. Generate a query embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b060c-352c-47c5-9fc6-1f156b1210fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Marking notebook endtime\n",
    "notebook_end_time = datetime.now(UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a62f2-6a59-4e30-88a1-d37d8cacf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from advanced_rag_utils import embedding_cost_report\n",
    "\n",
    "cost_for_notebook = get_bedrock_token_based_cost(model_id, notebook_start_time, notebook_end_time)\n",
    "\n",
    "# Your assumptions for your use case:\n",
    "scenario_number_of_documents = 100000\n",
    "scenario_number_of_queries =   5000000\n",
    " \n",
    "display(Markdown(embedding_cost_report(vector_store_embedding_cost, cost_for_notebook, scenario_number_of_documents, scenario_number_of_queries)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
