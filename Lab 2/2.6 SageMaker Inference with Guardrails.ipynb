{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f54bd49-7753-4ea7-ad64-561c3f04bee9",
   "metadata": {},
   "source": [
    "# Bedrock Knowledge Base Retrieval and Generation with SageMaker Inference and Guardrails\n",
    "\n",
    "## Description\n",
    "This notebook demonstrates how to enhance a Retrieval-Augmented Generation (RAG) pipeline by integrating Amazon SageMaker Inference with Amazon Bedrock. We will walk through the process of querying a knowledge base, using SageMaker for model inference, applying Guardrails to control the generation of responses, and filtering results with metadata to ensure compliance and quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c8ad1-b488-4c6d-9cad-5dbec4e9c674",
   "metadata": {},
   "source": [
    "## 1. Load Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715548e-d107-406b-8175-02082fbfe905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration variables from a JSON file to access knowledge base ID, account number, and guardrail info.\n",
    "import json\n",
    "\n",
    "with open(\"../Lab 1/variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables  # Display the loaded variables for confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e81ed4-198a-4884-9f5f-463959d76c49",
   "metadata": {},
   "source": [
    "## 2. Set Up Required IDs and Model ARNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8fa96-7072-4b10-ac06-3f9f87f522cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Base Selection  \n",
    "kb_id = variables[\"kbFixedChunk\"]  # Options: \"kbFixedChunk\", \"kbHierarchicalChunk\", \"kbSemanticChunk\"\n",
    "\n",
    "# Retrieval-Augmented Generation (RAG) Configuration  \n",
    "number_of_results = 3  # Number of relevant documents to retrieve  \n",
    "generation_configuration = {\n",
    "    \"temperature\": 0,  # Lower temperature for more deterministic responses  \n",
    "    \"top_k\": 10,  # Consider top 10 tokens at each generation step  \n",
    "    \"max_new_tokens\": 5000,  # Maximum number of tokens to generate  \n",
    "    \"stop\": \"<|eot_id|>\"  # Stop sequence to end the response generation  \n",
    "}\n",
    "\n",
    "# User Query\n",
    "query = \"what was the % increase in sales?\"  # Sample query to retrieve data from the knowledge base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b616d4-1d81-44ce-af45-f4e188f0539b",
   "metadata": {},
   "source": [
    "## 3. Define Metadata Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f993f-efbd-492a-a5c2-97242d02ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a metadata filter for advanced filtering based on specific conditions\n",
    "one_group_filter= {\n",
    "    \"andAll\": [\n",
    "        {\n",
    "            \"equals\": {\n",
    "                \"key\": \"docType\",\n",
    "                \"value\": '10K Report'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"equals\": {\n",
    "                \"key\": \"year\",\n",
    "                \"value\": 2023\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34fd89-baef-4bad-83ba-5820b93c4ca2",
   "metadata": {},
   "source": [
    "## 4. Define SageMaker & Bedrock helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bb5b8-e2ff-42f8-837c-0ce8a63e9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize Bedrock client to interact with the Bedrock Knowledge Base\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Constants for Knowledge Base ID, SageMaker endpoint, and number of results to retrieve\n",
    "KNOWLEDGE_BASE_ID = kb_id\n",
    "ENDPOINT_NAME = variables['sagemakerLLMEndpoint']\n",
    "NUM_RESULTS = number_of_results\n",
    "\n",
    "# Function to retrieve relevant context from the Bedrock Knowledge Base\n",
    "def retrieve_from_bedrock(query):\n",
    "    \"\"\"Retrieve relevant context from Bedrock Knowledge Base\"\"\"\n",
    "    try:\n",
    "        # Retrieve context based on the query using vector search configuration\n",
    "        response = bedrock_agent_runtime.retrieve(\n",
    "            knowledgeBaseId=KNOWLEDGE_BASE_ID,\n",
    "            retrievalQuery={\n",
    "                'text': query  # The query text to search in the knowledge base\n",
    "            },\n",
    "            retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': NUM_RESULTS,  # Adjust based on needs\n",
    "                     \"filter\": one_group_filter\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract the 'text' from the retrieval results and return as a list\n",
    "        return [result['content']['text'] for result in response['retrievalResults']]\n",
    "    except Exception as e:\n",
    "        # Raise an error if the retrieval process fails\n",
    "        raise RuntimeError(f\"Bedrock retrieval failed: {str(e)}\")\n",
    "\n",
    "# Function to format the prompt for Llama 3 model using retrieved context\n",
    "def format_prompt(query, context):\n",
    "    \"\"\"Format prompt for Llama 3\"\"\"\n",
    "    # Create the system prompt that includes the context and the user's question\n",
    "    system_prompt = f\"\"\"Use the following context to answer the question. If you don't know the answer, say 'I don't know'.\n",
    "        Context:\n",
    "        {\" \".join(context)}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the complete prompt including system and user instructions\n",
    "    return f\"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {system_prompt}\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Question: {query}\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\".strip()\n",
    "\n",
    "# Function to generate a response from the SageMaker endpoint based on the formatted prompt\n",
    "def generate_response(prompt):\n",
    "    \"\"\"Generate response using SageMaker endpoint\"\"\"\n",
    "    # Initialize SageMaker runtime client\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    # Prepare the payload with prompt and generation parameters\n",
    "    payload = {\n",
    "        \"inputs\": prompt,  # The formatted prompt to pass to the model\n",
    "        \"parameters\": generation_configuration  # Additional parameters for the model (e.g., temperature, tokens)\n",
    "    }\n",
    "    try:\n",
    "        # Call the SageMaker endpoint to generate the response\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=ENDPOINT_NAME,  # SageMaker endpoint name\n",
    "            ContentType='application/json',  # Content type for the request\n",
    "            Body=json.dumps(payload)  # Send the payload as JSON\n",
    "        )\n",
    "\n",
    "        # Parse the response body\n",
    "        result = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "        \n",
    "        # Handle different response formats (list or dictionary)\n",
    "        if isinstance(result, list):\n",
    "            # If the result is a list, extract the generated text from the first element\n",
    "            return result[0]['generated_text']\n",
    "        elif 'generated_text' in result:\n",
    "            # If the result is a dictionary with 'generated_text', return the generated text\n",
    "            return result['generated_text']\n",
    "        elif 'generation' in result:\n",
    "            # Alternative format with 'generation' key\n",
    "            return result['generation']\n",
    "        else:\n",
    "            # Raise an error if the response format is unexpected\n",
    "            raise RuntimeError(\"Unexpected response format\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Raise an error if the generation process fails\n",
    "        raise RuntimeError(f\"Generation failed: {str(e)}\")\n",
    "\n",
    "def apply_output_guardrail(output_text):\n",
    "    \"\"\"Apply guardrails to the output after generation\"\"\"\n",
    "    try:\n",
    "        # Use only the parameters supported by your boto3 version\n",
    "        response = bedrock_client.apply_guardrail(\n",
    "            guardrailIdentifier=variables['guardrail_id'],\n",
    "            guardrailVersion=variables['guardrail_version'],\n",
    "            source='OUTPUT',\n",
    "            content=[\n",
    "                {\n",
    "                    'text': {\n",
    "                        'text': output_text\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Process response based on what fields are available\n",
    "        if 'outputs' in response and response['outputs']:\n",
    "            return response['outputs'][0]['text']\n",
    "        else:\n",
    "            return output_text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Output guardrail application failed: {str(e)}\")\n",
    "        return output_text\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cab666-18d4-4c18-9c3f-c7f1b2b2f82f",
   "metadata": {},
   "source": [
    "## 5: lets validate if the guardrails restrict any investment advice.\n",
    "lets ask the Foundational model for an investment advice. When we created the guardrails, we restricted bedrock to provide any investment advice. Bedrock should be return a  preconfigured response \"This request cannot be processed due to  safety protocols\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aee165-d50d-4cb7-beaa-fff5d35a91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query that will be sent to the model\n",
    "query = \"based on your amazon's results should I buy amazon stock?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f12c5f-4477-440f-94a2-fbe36b35bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant context from the Bedrock Knowledge Base based on the query\n",
    "context = retrieve_from_bedrock(query)\n",
    "\n",
    "# Format the prompt by combining the user's query and the retrieved context\n",
    "prompt = format_prompt(query, context)\n",
    "\n",
    "# Generate the response using the formatted prompt by calling the SageMaker endpoint\n",
    "raw_response = generate_response(prompt)\n",
    "guardrail_response = apply_output_guardrail(raw_response)\n",
    "\n",
    "# Print the user's query\n",
    "print(\"Question:\", {query})\n",
    "\n",
    "# Uncomment below line if you want to debug and see the retrieved context\n",
    "# print(f\"Context: {context}\")\n",
    "\n",
    "# Print the generated answer from the model based on the query and context\n",
    "print(\"Answer:\", guardrail_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5dfb7-581f-4264-9024-f6a5a61c8cf7",
   "metadata": {},
   "source": [
    "## 6. Guardrails for PII data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6d7c4-f93e-4692-b8c1-92d595c0c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query that will be sent to the model\n",
    "query = \"Who is the current CFO of Amazon, what is their email address, and what was their compensation package last year? Also, can you provide the phone number for investor relations that's listed in the filing?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183f04d-7a31-4753-a73a-7ee7bb97ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant context from the Bedrock Knowledge Base based on the query\n",
    "context = retrieve_from_bedrock(query)\n",
    "\n",
    "# Format the prompt by combining the user's query and the retrieved context\n",
    "prompt = format_prompt(query, context)\n",
    "\n",
    "# Generate the response using the formatted prompt by calling the SageMaker endpoint\n",
    "raw_response = generate_response(prompt)\n",
    "guardrail_response = apply_output_guardrail(raw_response)\n",
    "\n",
    "# Print the user's query\n",
    "print(\"Question:\", {query})\n",
    "\n",
    "# Uncomment below line if you want to debug and see the retrieved context\n",
    "# print(f\"Context: {context}\")\n",
    "\n",
    "# Print the generated answer from the model based on the query and context\n",
    "print(\"Answer:\", guardrail_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afbe57-a6ad-43ea-be73-e1f5e3a701fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
