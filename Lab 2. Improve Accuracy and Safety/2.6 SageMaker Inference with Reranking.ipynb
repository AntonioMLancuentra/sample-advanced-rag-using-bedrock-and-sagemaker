{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f684c8-b863-4dc7-86b5-2d6632d3d7df",
   "metadata": {},
   "source": [
    "# Bedrock Knowledge Base Retrieval and Generation with SageMaker Inference and Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61f7ef",
   "metadata": {},
   "source": [
    "![Reranking](./reranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fef89-4586-4c15-a088-9b491fb7329b",
   "metadata": {},
   "source": [
    "## 1: Import and Load Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f2da5-8c58-4826-a216-fcd1c1c5cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the configuration variables from a JSON file\n",
    "with open(\"../variables.json\", \"r\") as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c726e87-7ada-412e-9d61-60dd3a0066c8",
   "metadata": {},
   "source": [
    "## 2: Define ARN and Configuration Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec1a90-e111-4077-a35b-26d620aaa7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Base Selection  \n",
    "kb_id = variables[\"kbFixedChunk\"]  # Options: \"kbFixedChunk\", \"kbHierarchicalChunk\", \"kbSemanticChunk\"\n",
    "\n",
    "# Retrieval-Augmented Generation (RAG) Configuration  \n",
    "number_of_results = 10  # Number of relevant documents to retrieve  \n",
    "generation_configuration = {\n",
    "    \"temperature\": 0,  # Lower temperature for more deterministic responses  \n",
    "    \"top_k\": 10,  # Consider top 10 tokens at each generation step  \n",
    "    \"max_new_tokens\": 5000,  # Maximum number of tokens to generate  \n",
    "    \"stop\": \"<|eot_id|>\"  # Stop sequence to end the response generation  \n",
    "}\n",
    "\n",
    "# Define ARNs (Amazon Resource Names) for the model\n",
    "rerank_model_arn=f\"arn:aws:bedrock:us-west-2::foundation-model/cohere.rerank-v3-5:0\"\n",
    "\n",
    "# User Query\n",
    "query = \"what was the % increase in sales?\"  # Sample query to retrieve data from the knowledge base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a73e5-d254-4acc-9f88-78e1b2a0bd69",
   "metadata": {},
   "source": [
    "## 3: Set Up Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6293a-1467-436d-962b-743f9f6d8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import *\n",
    "\n",
    "# Configure the Bedrock client\n",
    "bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name=\"us-west-2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551de863-30ea-4831-860e-37ab4db0913d",
   "metadata": {},
   "source": [
    "## 4: Define Function for Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cfc15-626b-4d02-b95a-ed87d975a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize Bedrock client to interact with the Bedrock Knowledge Base\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=variables[\"regionName\"])\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=variables[\"regionName\"])\n",
    "\n",
    "# Constants for Knowledge Base ID, SageMaker endpoint, and number of results to retrieve\n",
    "KNOWLEDGE_BASE_ID = kb_id\n",
    "ENDPOINT_NAME = variables['sagemakerLLMEndpoint']\n",
    "NUM_RESULTS = number_of_results\n",
    "\n",
    "def search_knowledge_base(query, region_name, kb_id, num_results=5, use_reranking=False, model_arn=None):\n",
    "    \"\"\"\n",
    "    Search Bedrock Knowledge Base and optionally rerank results.\n",
    "    Returns document texts and displays detailed metadata.\n",
    "    \"\"\"\n",
    "    client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "    \n",
    "    # 1. Retrieve from knowledge base\n",
    "    try:\n",
    "        kb_response = client.retrieve(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            retrievalQuery={\"text\": query},\n",
    "            retrievalConfiguration={\"vectorSearchConfiguration\": {\"numberOfResults\": num_results}}\n",
    "        )\n",
    "        \n",
    "        # Extract documents and metadata\n",
    "        documents = []\n",
    "        original_results = []\n",
    "        \n",
    "        for i, result in enumerate(kb_response.get(\"retrievalResults\", [])):\n",
    "            # Extract text from result\n",
    "            text = \"\"\n",
    "            if \"content\" in result and \"text\" in result[\"content\"]:\n",
    "                content_text = result[\"content\"][\"text\"]\n",
    "                if isinstance(content_text, list):\n",
    "                    text = \" \".join([item.get(\"span\", \"\") if isinstance(item, dict) else str(item) \n",
    "                                  for item in content_text])\n",
    "                else:\n",
    "                    text = str(content_text)\n",
    "                \n",
    "            # Store original result with metadata\n",
    "            original_results.append({\n",
    "                \"position\": i + 1,\n",
    "                \"score\": result.get(\"scoreValue\", 0),\n",
    "                \"text\": text[:300] + \"...\" if len(text) > 300 else text\n",
    "            })\n",
    "            documents.append(text)\n",
    "        \n",
    "        # Display original results\n",
    "        print(\"\\nTOP 3 DOCUMENTS WITHOUT RERANKING:\")\n",
    "        for doc in original_results[:min(3, len(original_results))]:\n",
    "            print(f\"Position {doc['position']} (Score: {doc['score']}):\")\n",
    "            print(f\"{doc['text']}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Search failed: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # 2. Rerank if enabled\n",
    "    if use_reranking and model_arn and documents:\n",
    "        try:\n",
    "            reranked = client.rerank(\n",
    "                queries=[{\"textQuery\": {\"text\": query}, \"type\": \"TEXT\"}],\n",
    "                rerankingConfiguration={\n",
    "                    \"bedrockRerankingConfiguration\": {\n",
    "                        \"modelConfiguration\": {\"modelArn\": model_arn},\n",
    "                        \"numberOfResults\": num_results\n",
    "                    },\n",
    "                    \"type\": \"BEDROCK_RERANKING_MODEL\"\n",
    "                },\n",
    "                sources=[{\n",
    "                    \"inlineDocumentSource\": {\"textDocument\": {\"text\": doc}, \"type\": \"TEXT\"},\n",
    "                    \"type\": \"INLINE\"\n",
    "                } for doc in documents]\n",
    "            )\n",
    "            \n",
    "            # Process reranked results\n",
    "            reranked_results = []\n",
    "            reranked_documents = []\n",
    "            \n",
    "            for new_pos, result in enumerate(reranked.get(\"results\", [])):\n",
    "                idx = result.get(\"index\", 0)\n",
    "                if 0 <= idx < len(documents):\n",
    "                    reranked_results.append({\n",
    "                        \"original_position\": idx + 1,\n",
    "                        \"new_position\": new_pos + 1,\n",
    "                        \"relevance_score\": result.get(\"relevanceScore\", 0),\n",
    "                        \"text\": documents[idx][:300] + \"...\" if len(documents[idx]) > 300 else documents[idx]\n",
    "                    })\n",
    "                    reranked_documents.append(documents[idx])\n",
    "            \n",
    "            # Display reranked results\n",
    "            print(\"\\nTOP 3 DOCUMENTS AFTER RERANKING:\")\n",
    "            for doc in reranked_results[:min(3, len(reranked_results))]:\n",
    "                print(f\"Moved from position {doc['original_position']} to {doc['new_position']}\")\n",
    "                print(f\"Relevance score: {doc['relevance_score']}\")\n",
    "                print(f\"{doc['text']}\\n\")\n",
    "            \n",
    "            return reranked_documents\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Reranking failed: {e}\")\n",
    "            print(\"Using original search results instead\")\n",
    "    \n",
    "    # Return document texts for format_prompt\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f161f-582e-4a18-8203-a8c02b61139e",
   "metadata": {},
   "source": [
    "## 5. Define SageMaker & Bedrock helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfb585-3169-4622-9dac-97e730cae767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the prompt for Llama 3 model using retrieved context\n",
    "def format_prompt(query, context):\n",
    "    \"\"\"\n",
    "    Format prompt for Llama 3 models in SageMaker using standard chat format.\n",
    "\n",
    "    Args:\n",
    "        query: The user's question to be answered\n",
    "        context: List of context documents retrieved from the knowledge base\n",
    "\n",
    "    Returns:\n",
    "        Formatted prompt string ready for model inference\n",
    "    \"\"\"\n",
    "    # Join all context documents into a single string\n",
    "    context_text = \" \".join(context)\n",
    "\n",
    "    # Limit context length to avoid token limits (approximate 6000 chars)\n",
    "    if len(context_text) > 6000:\n",
    "        context_text = context_text[:6000] + \"...\"\n",
    "\n",
    "    # Create system prompt with context\n",
    "    system_prompt = f\"\"\"Use the following context to answer the question. If you don't know the answer, say 'I don't know'.\n",
    "    Context:\n",
    "    {context_text}\n",
    "    \"\"\"\n",
    "\n",
    "    # Format prompt in Llama 3 chat format\n",
    "    prompt = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    {system_prompt}\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Question: {query}\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_response(prompt, endpoint_name=None):\n",
    "    \"\"\"\n",
    "    Generate response using SageMaker endpoint with robust error handling and response parsing.\n",
    "\n",
    "    Args:\n",
    "        prompt: The formatted prompt to send to the model\n",
    "        endpoint_name: Optional custom endpoint name (overrides ENDPOINT_NAME)\n",
    "\n",
    "    Returns:\n",
    "        Generated text response from the model\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    import json\n",
    "    import traceback\n",
    "\n",
    "    # Initialize SageMaker runtime client\n",
    "    runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "    # Use provided endpoint name or fall back to global variable\n",
    "    target_endpoint = endpoint_name if endpoint_name else variables['sagemakerLLMEndpoint']\n",
    "\n",
    "    # Prepare the payload with prompt and generation parameters\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": generation_configuration\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Call the SageMaker endpoint\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName=target_endpoint,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        # Parse the response body\n",
    "        response_body = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "\n",
    "        # Print raw response for debugging (can be removed in production)\n",
    "        print(\"Raw response:\")\n",
    "        print(json.dumps(response_body, indent=2)[:1000])  # Truncate if very large\n",
    "\n",
    "        # Handle different response formats\n",
    "        result_text = \"\"\n",
    "\n",
    "        if isinstance(response_body, list):\n",
    "            # List format (common in some models)\n",
    "            if len(response_body) > 0:\n",
    "                if 'generated_text' in response_body[0]:\n",
    "                    result_text = response_body[0]['generated_text']\n",
    "                else:\n",
    "                    # If no generated_text key, use the first item\n",
    "                    result_text = str(response_body[0])\n",
    "        elif isinstance(response_body, dict):\n",
    "            # Dictionary format\n",
    "            for possible_key in ['generated_text', 'generation', 'outputs', 'output', 'text']:\n",
    "                if possible_key in response_body:\n",
    "                    result_text = response_body[possible_key]\n",
    "                    break\n",
    "\n",
    "            # If none of the expected keys found\n",
    "            if not result_text and response_body:\n",
    "                print(f\"Unknown response format. Keys: {list(response_body.keys())}\")\n",
    "                # Try to use the full response as a fallback\n",
    "                result_text = str(response_body)\n",
    "        else:\n",
    "            # Any other format, convert to string\n",
    "            result_text = str(response_body)\n",
    "\n",
    "        # Check for empty response\n",
    "        if not result_text:\n",
    "            print(\"Warning: Empty response from model\")\n",
    "            result_text = \"The model returned an empty response. This might indicate an issue with the prompt format or model configuration.\"\n",
    "\n",
    "        return result_text\n",
    "\n",
    "    except Exception as e:\n",
    "        # Detailed error handling with traceback\n",
    "        error_msg = f\"Error generating response: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        print(traceback.format_exc())\n",
    "        return f\"Error in generation: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401c9d7-7208-4dc3-8ee7-54286ddd4fc0",
   "metadata": {},
   "source": [
    "## 6: Compare the Retrieved results WITH & WITHOUT Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580c3a5-566b-42a3-a1f9-ca0f08f75097",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Compare the results between 2022 and 2023\"\n",
    "\n",
    "print(\"WITHOUT RERANKING:\")\n",
    "context_without_reranking = search_knowledge_base(\n",
    "    query=query,\n",
    "    region_name=variables[\"regionName\"],\n",
    "    kb_id=variables[\"kbFixedChunk\"],\n",
    "    num_results=number_of_results,\n",
    "    use_reranking=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf8472-12e4-4628-a4fd-7ec6184768ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nWITH RERANKING:\")\n",
    "context_with_reranking = search_knowledge_base(\n",
    "    query=query,\n",
    "    region_name=variables[\"regionName\"],\n",
    "    kb_id=variables[\"kbFixedChunk\"],\n",
    "    num_results=number_of_results,\n",
    "    use_reranking=True,\n",
    "    model_arn=rerank_model_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba6463-ed89-4d6a-9352-d4a88eccc4f0",
   "metadata": {},
   "source": [
    "## 7: Compare the Generated results WITH & WITHOUT Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209784f-cb9d-4519-bda0-243f78fa5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITHOUT RERANKING:\")\n",
    "\n",
    "# Format the prompt by combining the user's query and the retrieved context\n",
    "prompt_without_reranking = format_prompt(query, context_without_reranking)\n",
    "\n",
    "# Generate the response using the formatted prompt\n",
    "response_without_reranking = generate_response(prompt_without_reranking)\n",
    "\n",
    "# Print the user's query and answer\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", response_without_reranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73a394-e973-4d5a-b1d8-933534be7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WITH RERANKING:\")\n",
    "\n",
    "# Format the prompt by combining the user's query and the retrieved context\n",
    "prompt_with_reranking = format_prompt(query, context_with_reranking)\n",
    "\n",
    "# Generate the response using the formatted prompt\n",
    "response_with_reranking = generate_response(prompt_with_reranking)\n",
    "\n",
    "# Print the user's query and answer\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", response_with_reranking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
